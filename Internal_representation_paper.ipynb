{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import ast\n",
    "import sqlite3\n",
    "from DeepCCS.model.encoders import SmilesToOneHotEncoder\n",
    "from DeepCCS.model.splitter import SMILESsplitter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import keras\n",
    "from keras.optimizers import adam, rmsprop\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, Concatenate\n",
    "from keras.layers import Conv1D, MaxPooling1D,  Activation, BatchNormalization, Flatten\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HMDB_SQLITE_FILE = \"hmdb_metabolites.sql\"\n",
    "datafile = \"DATASETS.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import median_absolute_error, mean_absolute_error, r2_score, mean_squared_error\n",
    "def relative_mean(Y_true, Y_pred):\n",
    "    mean = np.mean((abs(Y_pred - Y_true) / Y_true) * 100)\n",
    "    return mean\n",
    "\n",
    "def relative_median(Y_true, Y_pred):\n",
    "    med = np.median((abs(Y_pred - Y_true) / Y_true) * 100)\n",
    "    return med\n",
    "\n",
    "def percentile_95(Y_true, Y_pred):\n",
    "    percentile = np.percentile((abs(Y_pred - Y_true) / Y_true) * 100, 95)\n",
    "    return percentile\n",
    "\n",
    "def percentile_90(Y_true, Y_pred):\n",
    "    percentile = np.percentile((abs(Y_pred - Y_true) / Y_true) * 100, 90)\n",
    "    return percentile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use an sqlite version of the original HMDB.xml file. It's easier to parse and re-use. Parsing script is HMDB_sql_converter.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_connection = sqlite3.connect(HMDB_SQLITE_FILE)\n",
    "cursor = sql_connection.cursor()\n",
    "SMILES = []\n",
    "polar_surface_area = []\n",
    "logS = []\n",
    "refractivity = []\n",
    "polarizability = []\n",
    "logP_alogps = []\n",
    "logP_chemaxon = []\n",
    "query = \"SELECT DISTINCT SMILES, polar_surface_area, logS, refractivity, polarizability, \\\n",
    "logP_ALOGPS, logP_ChemAxon \\\n",
    "FROM HMDB where SMILES is not null and \\\n",
    "polar_surface_area is not null and \\\n",
    "logS is not null and \\\n",
    "refractivity is not null and \\\n",
    "polarizability is not null and \\\n",
    "logP_ALOGPS is not null and \\\n",
    "logP_ChemAxon is not null;\"\n",
    "for i in cursor.execute(query):\n",
    "    SMILES.append(i[0])\n",
    "    polar_surface_area.append(float(i[1]))\n",
    "    logS.append(float(i[2]))\n",
    "    refractivity.append(float(i[3]))\n",
    "    polarizability.append(float(i[4]))\n",
    "    logP_alogps.append(float(i[5]))\n",
    "    logP_chemaxon.append(float(i[6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now filter the dataset to keep only SMILES with an appropriate length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = [np.array(polar_surface_area), np.array(logS), np.array(refractivity), np.array(polarizability),\n",
    "     np.array(logP_alogps), np.array(logP_chemaxon)]\n",
    "X = np.array(SMILES)\n",
    "\n",
    "# Filter SMILES by length\n",
    "smiles_splitter_multi = SMILESsplitter()\n",
    "lengths = [len(smiles_splitter_multi.split(x)) for x in X]\n",
    "lengths_filter = np.array(lengths) <= 250\n",
    "\n",
    "X = X[lengths_filter.astype(bool)]\n",
    "Y = [target[lengths_filter.astype(bool)] for target in Y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective is to measure the impact of learning the internal representation using a multi-output problem and to re-use the internal representation to predict CCS. To make sure the SMILES encoder will be compatible with both problems, we will train a SMILES encoder that is compatible with both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_encoder_ccs = SmilesToOneHotEncoder()\n",
    "smiles_encoder_ccs.load_encoder(\"SMILES_encoder.json\")\n",
    "\n",
    "smiles_encoder_multi = SmilesToOneHotEncoder()\n",
    "smiles_encoder_multi.fit(X)\n",
    "\n",
    "all_symbols = smiles_encoder_ccs.converter.keys() + smiles_encoder_multi.converter.keys()\n",
    "\n",
    "smiles_encoder_multi = SmilesToOneHotEncoder()\n",
    "for i, j in enumerate(set(all_symbols)):\n",
    "    smiles_encoder_multi.converter[j] = i\n",
    "smiles_encoder_multi._is_fit = True\n",
    "\n",
    "print(\"There is {} symbols in the SMILES encoder\".format(len(smiles_encoder_multi.converter)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a SMILES encoder, a bunch of SMILES and a series of properties to predict.\n",
    "We have to seperate the data between a train, valid and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train - test\n",
    "np.random.seed(432)\n",
    "mask_train = np.zeros(len(X), dtype=int)\n",
    "mask_train[:int(len(X) * 0.7)] = 1\n",
    "np.random.shuffle(mask_train)\n",
    "mask_test = 1 - mask_train\n",
    "mask_train = mask_train.astype(bool)\n",
    "mask_test = mask_test.astype(bool)\n",
    "\n",
    "X_pooled = X[mask_train]\n",
    "X_test = X[mask_test]\n",
    "\n",
    "Y_pooled = [i[mask_train] for i in Y]\n",
    "Y_test = [i[mask_test] for i in Y]\n",
    "\n",
    "# Split train - valid\n",
    "mask_train = np.zeros(len(X_pooled), dtype=int)\n",
    "mask_train[:int(len(X_pooled) * 0.9)] = 1\n",
    "np.random.shuffle(mask_train)\n",
    "mask_valid = 1 - mask_train\n",
    "mask_train = mask_train.astype(bool)\n",
    "mask_valid = mask_valid.astype(bool)\n",
    "\n",
    "X_train = X_pooled[mask_train]\n",
    "X_valid = X_pooled[mask_valid]\n",
    "\n",
    "Y_train = [i[mask_train] for i in Y_pooled]\n",
    "Y_valid = [i[mask_valid] for i in Y_pooled]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to encode the SMILES with our SMILES encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SMILES encoding ----> To execute only if you have enough RAM power (in general, not a standard PC RAM)\n",
    "# X_train_encoded = smiles_encoder_multi.transform(X_train)\n",
    "# X_valid_encoded = smiles_encoder_multi.transform(X_valid)\n",
    "# X_test_encoded = smiles_encoder_multi.transform(X_test)\n",
    "# smiles_encoder_multi.save_encoder(\"Smiles_encoder_multi.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now implement the network's structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Network structure\n",
    "smile_input_layer = Input(shape=(250, len(smiles_encoder_multi.converter)), name=\"smiles\")\n",
    "conv = Conv1D(64, kernel_size=4, activation='relu', kernel_initializer='normal')(smile_input_layer)\n",
    "\n",
    "previous = conv\n",
    "for i in range(6):\n",
    "    conv = Conv1D(64, kernel_size=4, activation='relu', kernel_initializer='normal')(previous)\n",
    "    if i == 5:\n",
    "        pool = MaxPooling1D(pool_size=2, strides=2)(conv)\n",
    "    else:\n",
    "        pool = MaxPooling1D(pool_size=2, strides=1)(conv)\n",
    "    previous = pool\n",
    "\n",
    "flat = Flatten()(previous)\n",
    "\n",
    "# polar_surface_area\n",
    "previous = flat\n",
    "for i in range(2):\n",
    "    dense_layer = Dense(384, activation=\"relu\", kernel_initializer='normal')(previous)\n",
    "    previous = dense_layer\n",
    "output_logp = Dense(1, activation=\"linear\", name=\"polar_surface_area\")(previous)\n",
    "\n",
    "# logS\n",
    "previous = flat\n",
    "for i in range(2):\n",
    "    dense_layer = Dense(384, activation=\"relu\", kernel_initializer='normal')(previous)\n",
    "    previous = dense_layer\n",
    "output_logs = Dense(1, activation=\"linear\", name=\"logs\")(previous)\n",
    "\n",
    "# refractivity\n",
    "\n",
    "previous = flat\n",
    "for i in range(2):\n",
    "    dense_layer = Dense(384, activation=\"relu\", kernel_initializer='normal')(previous)\n",
    "    previous = dense_layer\n",
    "output_refractivity = Dense(1, activation=\"linear\", name=\"refractivity\")(previous)\n",
    "\n",
    "# polarizability\n",
    "previous = flat\n",
    "for i in range(2):\n",
    "    dense_layer = Dense(384, activation=\"relu\", kernel_initializer='normal')(previous)\n",
    "    previous = dense_layer\n",
    "output_polarizability = Dense(1, activation=\"linear\", name=\"polarizability\")(previous)\n",
    "\n",
    "#logP_alogps\n",
    "previous = flat\n",
    "for i in range(2):\n",
    "    dense_layer = Dense(384, activation=\"relu\", kernel_initializer='normal')(previous)\n",
    "    previous = dense_layer\n",
    "output_logp_alogps = Dense(1, activation=\"linear\", name=\"logp_alogps\")(previous)\n",
    "\n",
    "#Logp_Chemaxon\n",
    "previous = flat\n",
    "for i in range(2):\n",
    "    dense_layer = Dense(384, activation=\"relu\", kernel_initializer='normal')(previous)\n",
    "    previous = dense_layer\n",
    "output_logp_chemaxon = Dense(1, activation=\"linear\", name=\"logp_chemaxon\")(previous)\n",
    "\n",
    "# optimizer and compile\n",
    "opt = getattr(keras.optimizers, 'adam')\n",
    "opt = opt()\n",
    "model = Model(input=smile_input_layer,\n",
    "              outputs=[output_logp, output_logs, output_refractivity, output_polarizability, \n",
    "                       output_logp_alogps, output_logp_chemaxon])\n",
    "model.compile(optimizer=opt, loss='mean_squared_error')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = \"Model_multioutput_2018-11-28-001.model\"\n",
    "model.load_weights(model_file)\n",
    "y_pred = model.predict(X_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = [\"polar_surface_area\", \"logS\", \"refractivity\", \"polarizability\", \"logP_alogps\", \"logP_chemaxon\"]\n",
    "for i, p in enumerate(properties):\n",
    "    print(p)\n",
    "    mse = mean_squared_error(y_true=Y_test[i], y_pred = y_pred[i])\n",
    "    r2 = r2_score(y_true=Y_test[i], y_pred = y_pred[i])\n",
    "    mean_abs_err = mean_absolute_error(y_true=Y_test[i], y_pred = y_pred[i])\n",
    "    median_abs_err = median_absolute_error(y_true=Y_test[i], y_pred = y_pred[i])\n",
    "    relative_mean_err = relative_mean(Y_test[i], y_pred[i].flatten())\n",
    "    relative_median_err = relative_median(Y_test[i], y_pred[i].flatten())\n",
    "    ninety = percentile_90(Y_test[i], y_pred[i].flatten())\n",
    "    ninety_five = percentile_95(Y_test[i], y_pred[i].flatten())\n",
    "    print(\"MSE: {}\".format(mse))\n",
    "    print(\"R2: {}\".format(r2))\n",
    "    print(\"median absolute: {}\".format(median_abs_err))\n",
    "    print(\"mean absolute: {}\".format(mean_abs_err))\n",
    "    print(\"median relative: {}\".format(relative_median_err))\n",
    "    print(\"mean relative: {}\".format(relative_mean_err))\n",
    "    print(\"90 %: {}\".format(ninety))\n",
    "    print(\"95 %: {}\".format(ninety_five))\n",
    "    print(\"------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain model for CCS prediction\n",
    "Now that the multi-output model can perform predictions for multiple properties, we will use its internal representation to perform CCS prediction. This will tell us if we have enough data to train the internal representation for CCS predictions using only the CCS data. Furthermore, it will show that CNN are appropriate for molecular descriptor predictions directly from SMILES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the data exactly the same way as it was done for the first experiment\n",
    "from DeepCCS.utils import read_dataset, filter_data\n",
    "from DeepCCS.model.encoders import AdductToOneHotEncoder\n",
    "\n",
    "datasets_names = [\"MetCCS_train_pos\", \"MetCCS_train_neg\", \"MetCCS_test_pos\", \"MetCCS_test_neg\", \n",
    "                  \"Astarita_pos\", \"Astarita_neg\", \"Baker\", \"McLean\", \"CBM\"] \n",
    "\n",
    "datasets = [read_dataset(datafile, d_name) for d_name in datasets_names] # Read\n",
    "datasets = [filter_data(d_set) for d_set in datasets] # Filter\n",
    "\n",
    "np.random.seed(777)\n",
    "save_test_sets_data = {}\n",
    "pooled_set = []\n",
    "test_sets = []\n",
    "train_set = []\n",
    "validation_set = []\n",
    "test_sets_names = []\n",
    "for i, dset in enumerate(datasets_names):\n",
    "    if dset in [\"MetCCS_train_pos\", \"MetCCS_train_neg\"]:\n",
    "        pooled_set.append([np.array(datasets[i][\"SMILES\"]),\n",
    "                           np.array(datasets[i][\"Adducts\"]),\n",
    "                           np.array(datasets[i][\"CCS\"])])\n",
    "    elif dset in [\"MetCCS_test_pos\", \"MetCCS_test_neg\", \"Astarita_pos\", \"Astarita_neg\"]:\n",
    "        test_sets.append([np.array(datasets[i][\"SMILES\"]),\n",
    "                          np.array(datasets[i][\"Adducts\"]),\n",
    "                          np.array(datasets[i][\"CCS\"])])\n",
    "        test_sets_names.append(dset)\n",
    "    elif dset in [\"Baker\", \"McLean\", \"CBM\"]:\n",
    "        smiles = np.array(datasets[i][\"SMILES\"])\n",
    "        ccs = np.array(datasets[i][\"CCS\"])\n",
    "        adducts = np.array(datasets[i][\"Adducts\"])\n",
    "        \n",
    "        # We use binary masks to split the datasets between pooled and test\n",
    "        mask_pooled = np.zeros(len(smiles), dtype=int)\n",
    "        mask_pooled[:int(len(smiles) * 0.8)] = 1  # The remaining 20% goes in the test set.\n",
    "        np.random.shuffle(mask_pooled)\n",
    "        mask_test = 1 - mask_pooled\n",
    "        mask_pooled = mask_pooled.astype(bool)\n",
    "        mask_test = mask_test.astype(bool)\n",
    "        \n",
    "        pooled_set.append([smiles[mask_pooled], adducts[mask_pooled], ccs[mask_pooled]])\n",
    "        test_sets.append([smiles[mask_test], adducts[mask_test], ccs[mask_test]])\n",
    "        test_sets_names.append(dset)\n",
    "# Split pooled between train (90%) and validation (10%)\n",
    "smiles_pooled = np.concatenate([i[0] for i in pooled_set])\n",
    "adducts_pooled = np.concatenate([i[1] for i in pooled_set])\n",
    "ccs_pooled = np.concatenate([i[2] for i in pooled_set])\n",
    "\n",
    "mask_train = np.zeros(len(smiles_pooled), dtype=int)\n",
    "mask_train[:int(len(smiles_pooled) * 0.9)] = 1  # The remaining 10% goes in the validation set.\n",
    "np.random.shuffle(mask_train)\n",
    "mask_valid = 1 - mask_train\n",
    "mask_train = mask_train.astype(bool)\n",
    "mask_valid = mask_valid.astype(bool)\n",
    "\n",
    "train_set = [smiles_pooled[mask_train], adducts_pooled[mask_train], ccs_pooled[mask_train]]\n",
    "validation_set = [smiles_pooled[mask_valid], adducts_pooled[mask_valid], ccs_pooled[mask_valid]]\n",
    "\n",
    "#We use the SMILES encoder that was used for encoding the HMDB database.\n",
    "train_set[0] = smiles_encoder_multi.transform(train_set[0])\n",
    "validation_set[0] = smiles_encoder_multi.transform(validation_set[0])\n",
    "\n",
    "adducts_encoder = AdductToOneHotEncoder()\n",
    "adducts_encoder.fit(np.concatenate([dset[\"Adducts\"] for dset in datasets]))\n",
    "train_set[1] = adducts_encoder.transform(train_set[1])\n",
    "validation_set[1] = adducts_encoder.transform(validation_set[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the input layer and the conv. + max. pooling layers + the flatten layer\n",
    "del model.layers[15:]\n",
    "\n",
    "# Set trainable property at false to lock weights\n",
    "for l in model.layers:\n",
    "    l.trainable = False\n",
    "\n",
    "    \n",
    "adduct_input_layer = Input(shape=(len(adducts_encoder.converter),), name=\"adduct\")\n",
    "\n",
    "previous = model.layers[-1].output\n",
    "\n",
    "remix_layer = keras.layers.concatenate([previous, adduct_input_layer], axis=-1)\n",
    "previous = remix_layer\n",
    "\n",
    "# Insert new dense layers\n",
    "for i in range(2):\n",
    "    dense_layer = Dense(384, activation=\"relu\", kernel_initializer='normal')(previous)\n",
    "    previous = dense_layer\n",
    "\n",
    "#Insert new ouput layer\n",
    "output = Dense(1, activation=\"linear\")(previous)\n",
    "\n",
    "\n",
    "# Compile\n",
    "opt = adam(lr=0.0001)\n",
    "new_model = Model(inputs=[model.layers[0].input, adduct_input_layer], outputs=output)\n",
    "new_model.compile(optimizer=opt, loss='mean_squared_error')\n",
    "\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the pure keras model to as DeepCCS model (done manually)\n",
    "from DeepCCS.model.DeepCCS import DeepCCSModel\n",
    "\n",
    "deepCCS_model = DeepCCSModel()\n",
    "deepCCS_model.adduct_encoder = adducts_encoder\n",
    "deepCCS_model.smiles_encoder = smiles_encoder_multi\n",
    "deepCCS_model.model = new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a global test set (for overall performances)\n",
    "test_smiles_global = np.concatenate([t[0] for t in test_sets])\n",
    "test_adducts_global = np.concatenate([t[1] for t in test_sets])\n",
    "test_ccs_global = np.concatenate([t[2] for t in test_sets])\n",
    "test_sets.append([test_smiles_global, test_adducts_global, test_ccs_global])\n",
    "test_sets_names = test_sets_names + [\"global\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = \"Model_CCS_after_multioutput_20180912.h5\"\n",
    "deepCCS_model.model.load_weights(model_file)\n",
    "for i, t in enumerate(test_sets):\n",
    "    predictions = deepCCS_model.predict(t[0], t[1]).flatten()\n",
    "    print(test_sets_names[i])\n",
    "    print(\"R2: {}\".format(r2_score(y_true=t[2], y_pred=predictions)))\n",
    "    print(\"Mean relative: {}\".format(relative_mean(Y_true=t[2], Y_pred=predictions)))\n",
    "    print(\"Median relative: {}\".format(relative_median(Y_true=t[2], Y_pred=predictions)))\n",
    "    print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

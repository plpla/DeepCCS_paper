{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import ast\n",
    "import sqlite3\n",
    "from DeepCCS.model.encoders import SmilesToOneHotEncoder\n",
    "from DeepCCS.model.splitter import SMILESsplitter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import keras\n",
    "from keras.optimizers import adam, rmsprop\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, Concatenate\n",
    "from keras.layers import Conv1D, MaxPooling1D,  Activation, BatchNormalization, Flatten\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HMDB_SQLITE_FILE = \"hmdb_metabolites.sql\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use an sqlite version of the original HMDB.xml file. It's easier to parse and re-use. Parsing script is HMDB_sql_converter.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_connection = sqlite3.connect(HMDB_SQLITE_FILE)\n",
    "cursor = sql_connection.cursor()\n",
    "SMILES = []\n",
    "polar_surface_area = []\n",
    "logS = []\n",
    "refractivity = []\n",
    "polarizability = []\n",
    "logP_alogps = []\n",
    "logP_chemaxon = []\n",
    "query = \"SELECT DISTINCT SMILES, polar_surface_area, logS, refractivity, polarizability, \\\n",
    "logP_ALOGPS, logP_ChemAxon \\\n",
    "FROM HMDB where SMILES is not null and \\\n",
    "polar_surface_area is not null and \\\n",
    "logS is not null and \\\n",
    "refractivity is not null and \\\n",
    "polarizability is not null and \\\n",
    "logP_ALOGPS is not null and \\\n",
    "logP_ChemAxon is not null;\"\n",
    "for i in cursor.execute(query):\n",
    "    SMILES.append(i[0])\n",
    "    polar_surface_area.append(float(i[1]))\n",
    "    logS.append(float(i[2]))\n",
    "    refractivity.append(float(i[3]))\n",
    "    polarizability.append(float(i[4]))\n",
    "    logP_alogps.append(float(i[5]))\n",
    "    logP_chemaxon.append(float(i[6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now filter the dataset to keep only SMILES with an appropriate length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = [np.array(polar_surface_area), np.array(logS), np.array(refractivity), np.array(polarizability),\n",
    "     np.array(logP_alogps), np.array(logP_chemaxon)]\n",
    "X = np.array(SMILES)\n",
    "\n",
    "# Filter SMILES by length\n",
    "smiles_splitter_multi = SMILESsplitter()\n",
    "lengths = [len(smiles_splitter_multi.split(x)) for x in X]\n",
    "lengths_filter = np.array(lengths) <= 250\n",
    "\n",
    "X = X[lengths_filter.astype(bool)]\n",
    "Y = [target[lengths_filter.astype(bool)] for target in Y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective is to measure the impact of learning the internal representation using a multi-output problem and to re-use the internal representation to predict CCS. To make sure the SMILES encoder will be compatible with both problems, we will train a SMILES encoder that is compatible with both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 58 symbols in the SMILES encoder\n"
     ]
    }
   ],
   "source": [
    "smiles_encoder_ccs = SmilesToOneHotEncoder()\n",
    "smiles_encoder_ccs.load_encoder(\"SMILES_encoder.json\")\n",
    "\n",
    "smiles_encoder_multi = SmilesToOneHotEncoder()\n",
    "smiles_encoder_multi.fit(X)\n",
    "\n",
    "all_symbols = smiles_encoder_ccs.converter.keys() + smiles_encoder_multi.converter.keys()\n",
    "#all_symbols = set(all_symbols)\n",
    "\n",
    "smiles_encoder_multi = SmilesToOneHotEncoder()\n",
    "for i, j in enumerate(set(all_symbols)):\n",
    "    smiles_encoder_multi.converter[j] = i\n",
    "smiles_encoder_multi._is_fit = True\n",
    "\n",
    "print(\"There is {} symbols in the SMILES encoder\".format(len(smiles_encoder_multi.converter)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a SMILES encoder, a bunch of SMILES and a series of properties to predict.\n",
    "We have to seperate the data between a train, valid and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train - test\n",
    "np.random.seed(432)\n",
    "mask_train = np.zeros(len(X), dtype=int)\n",
    "mask_train[:int(len(X) * 0.7)] = 1\n",
    "np.random.shuffle(mask_train)\n",
    "mask_test = 1 - mask_train\n",
    "mask_train = mask_train.astype(bool)\n",
    "mask_test = mask_test.astype(bool)\n",
    "\n",
    "X_pooled = X[mask_train]\n",
    "X_test = X[mask_test]\n",
    "\n",
    "Y_pooled = [i[mask_train] for i in Y]\n",
    "Y_test = [i[mask_test] for i in Y]\n",
    "\n",
    "# Split train - valid\n",
    "mask_train = np.zeros(len(X_pooled), dtype=int)\n",
    "mask_train[:int(len(X_pooled) * 0.9)] = 1\n",
    "np.random.shuffle(mask_train)\n",
    "mask_valid = 1 - mask_train\n",
    "mask_train = mask_train.astype(bool)\n",
    "mask_valid = mask_valid.astype(bool)\n",
    "\n",
    "X_train = X_pooled[mask_train]\n",
    "X_valid = X_pooled[mask_valid]\n",
    "\n",
    "Y_train = [i[mask_train] for i in Y_pooled]\n",
    "Y_valid = [i[mask_valid] for i in Y_pooled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMILES encoding\n",
    "X_train_encoded = smiles_encoder_multi.transform(X_train)\n",
    "X_valid_encoded = smiles_encoder_multi.transform(X_valid)\n",
    "X_test_encoded = smiles_encoder_multi.transform(X_test)\n",
    "smiles_encoder_multi.save_encoder(\"Smiles_encoder_multi.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pplante/tensorflow-py2.7.13-v2/lib/python2.7/site-packages/ipykernel_launcher.py:64: UserWarning:\n",
      "\n",
      "Update your `Model` call to the Keras 2 API: `Model(outputs=[<tf.Tenso..., inputs=Tensor(\"sm...)`\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "smiles (InputLayer)             (None, 250, 58)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 247, 64)      14912       smiles[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 244, 64)      16448       conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 243, 64)      0           conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 240, 64)      16448       max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 239, 64)      0           conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 236, 64)      16448       max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 235, 64)      0           conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 232, 64)      16448       max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 231, 64)      0           conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 228, 64)      16448       max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 227, 64)      0           conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 224, 64)      16448       max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 112, 64)      0           conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 7168)         0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_46 (Dense)                (None, 384)          2752896     flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_48 (Dense)                (None, 384)          2752896     flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_50 (Dense)                (None, 384)          2752896     flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_52 (Dense)                (None, 384)          2752896     flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_54 (Dense)                (None, 384)          2752896     flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_56 (Dense)                (None, 384)          2752896     flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_47 (Dense)                (None, 384)          147840      dense_46[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_49 (Dense)                (None, 384)          147840      dense_48[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_51 (Dense)                (None, 384)          147840      dense_50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_53 (Dense)                (None, 384)          147840      dense_52[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_55 (Dense)                (None, 384)          147840      dense_54[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_57 (Dense)                (None, 384)          147840      dense_56[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "polar_surface_area (Dense)      (None, 1)            385         dense_47[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "logs (Dense)                    (None, 1)            385         dense_49[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "refractivity (Dense)            (None, 1)            385         dense_51[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "polarizability (Dense)          (None, 1)            385         dense_53[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "logp_alogps (Dense)             (None, 1)            385         dense_55[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "logp_chemaxon (Dense)           (None, 1)            385         dense_57[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 17,520,326\n",
      "Trainable params: 17,520,326\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Network structure\n",
    "smile_input_layer = Input(shape=(250, len(smiles_encoder_multi.converter)), name=\"smiles\")\n",
    "conv = Conv1D(64, kernel_size=4, activation='relu', kernel_initializer='normal')(smile_input_layer)\n",
    "\n",
    "previous = conv\n",
    "for i in range(6):\n",
    "    conv = Conv1D(64, kernel_size=4, activation='relu', kernel_initializer='normal')(previous)\n",
    "    if i == 5:\n",
    "        pool = MaxPooling1D(pool_size=2, strides=2)(conv)\n",
    "    else:\n",
    "        pool = MaxPooling1D(pool_size=2, strides=1)(conv)\n",
    "    previous = pool\n",
    "\n",
    "flat = Flatten()(previous)\n",
    "\n",
    "# polar_surface_area\n",
    "previous = flat\n",
    "for i in range(2):\n",
    "    dense_layer = Dense(384, activation=\"relu\", kernel_initializer='normal')(previous)\n",
    "    previous = dense_layer\n",
    "output_logp = Dense(1, activation=\"linear\", name=\"polar_surface_area\")(previous)\n",
    "\n",
    "# logS\n",
    "previous = flat\n",
    "for i in range(2):\n",
    "    dense_layer = Dense(384, activation=\"relu\", kernel_initializer='normal')(previous)\n",
    "    previous = dense_layer\n",
    "output_logs = Dense(1, activation=\"linear\", name=\"logs\")(previous)\n",
    "\n",
    "# refractivity\n",
    "\n",
    "previous = flat\n",
    "for i in range(2):\n",
    "    dense_layer = Dense(384, activation=\"relu\", kernel_initializer='normal')(previous)\n",
    "    previous = dense_layer\n",
    "output_refractivity = Dense(1, activation=\"linear\", name=\"refractivity\")(previous)\n",
    "\n",
    "# polarizability\n",
    "previous = flat\n",
    "for i in range(2):\n",
    "    dense_layer = Dense(384, activation=\"relu\", kernel_initializer='normal')(previous)\n",
    "    previous = dense_layer\n",
    "output_polarizability = Dense(1, activation=\"linear\", name=\"polarizability\")(previous)\n",
    "\n",
    "#logP_alogps\n",
    "previous = flat\n",
    "for i in range(2):\n",
    "    dense_layer = Dense(384, activation=\"relu\", kernel_initializer='normal')(previous)\n",
    "    previous = dense_layer\n",
    "output_logp_alogps = Dense(1, activation=\"linear\", name=\"logp_alogps\")(previous)\n",
    "\n",
    "#Logp_Chemaxon\n",
    "previous = flat\n",
    "for i in range(2):\n",
    "    dense_layer = Dense(384, activation=\"relu\", kernel_initializer='normal')(previous)\n",
    "    previous = dense_layer\n",
    "output_logp_chemaxon = Dense(1, activation=\"linear\", name=\"logp_chemaxon\")(previous)\n",
    "\n",
    "# optimizer and compile\n",
    "opt = getattr(keras.optimizers, 'adam')\n",
    "opt = opt()\n",
    "model = Model(input=smile_input_layer,\n",
    "              outputs=[output_logp, output_logs, output_refractivity, output_polarizability, \n",
    "                       output_logp_alogps, output_logp_chemaxon])\n",
    "model.compile(optimizer=opt, loss='mean_squared_error')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = \"Model_multioutput_2018-09-11-001.model\"\n",
    "model_checkpoint = ModelCheckpoint(model_file, save_best_only=True, save_weights_only=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_encoded, Y_train,\n",
    "          epochs=150,\n",
    "          batch_size=20,\n",
    "          validation_data=(X_valid_encoded, Y_valid),\n",
    "          verbose=False,\n",
    "          callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(model_file)\n",
    "y_pred = model.predict(X_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polar_surface_area\n",
      "MSE: 1.19446786589\n",
      "R2: 0.999833217352\n",
      "median absolute: 0.219766235352\n",
      "mean absolute: 0.439682208361\n",
      "median relative: 0.166994711849\n",
      "mean relative: inf\n",
      "90 %: 1.02826103388\n",
      "95 %: 2.0255404024\n",
      "------------------\n",
      "logS\n",
      "MSE: 0.0648184888949\n",
      "R2: 0.985988465844\n",
      "median absolute: 0.0467190170288\n",
      "mean absolute: 0.124112965671\n",
      "median relative: -0.631134927949\n",
      "mean relative: nan\n",
      "90 %: -0.136416316491\n",
      "95 %: -0.0631695144747\n",
      "------------------\n",
      "refractivity\n",
      "MSE: 1.60145042669\n",
      "R2: 0.999868608645\n",
      "median absolute: 0.270885009766\n",
      "mean absolute: 0.564469209383\n",
      "median relative: 0.0995166920787\n",
      "mean relative: 0.614417425037\n",
      "90 %: 1.37429328224\n",
      "95 %: 2.48349960618\n",
      "------------------\n",
      "polarizability\n",
      "MSE: 0.630959301337\n",
      "R2: 0.999736535065\n",
      "median absolute: 0.223478775024\n",
      "mean absolute: 0.448532211803\n",
      "median relative: 0.200520334282\n",
      "mean relative: 0.837716418313\n",
      "90 %: 2.33731473719\n",
      "95 %: 3.80857962149\n",
      "------------------\n",
      "logP_alogps\n",
      "MSE: 0.144497530144\n",
      "R2: 0.989050554481\n",
      "median absolute: 0.0639659881592\n",
      "mean absolute: 0.178734421172\n",
      "median relative: 0.641853304157\n",
      "mean relative: nan\n",
      "90 %: 15.2964144704\n",
      "95 %: 35.9931477479\n",
      "------------------\n",
      "logP_chemaxon\n",
      "MSE: 0.287028207713\n",
      "R2: 0.996627091385\n",
      "median absolute: 0.0395975494385\n",
      "mean absolute: 0.206257297058\n",
      "median relative: 0.157443124814\n",
      "mean relative: -5.98708023129\n",
      "90 %: 13.0610599146\n",
      "95 %: 33.2790904575\n",
      "------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pplante/tensorflow-py2.7.13-v2/lib/python2.7/site-packages/ipykernel_launcher.py:3: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in divide\n",
      "\n",
      "/home/pplante/tensorflow-py2.7.13-v2/lib/python2.7/site-packages/ipykernel_launcher.py:8: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in divide\n",
      "\n",
      "/home/pplante/tensorflow-py2.7.13-v2/lib/python2.7/site-packages/ipykernel_launcher.py:16: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in divide\n",
      "\n",
      "/home/pplante/tensorflow-py2.7.13-v2/lib/python2.7/site-packages/ipykernel_launcher.py:12: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in divide\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import median_absolute_error, mean_absolute_error, r2_score, mean_squared_error\n",
    "def relative_mean(Y_true, Y_pred):\n",
    "    mean = np.mean((abs(Y_pred - Y_true) / Y_true) * 100)\n",
    "    return mean\n",
    "\n",
    "\n",
    "def relative_median(Y_true, Y_pred):\n",
    "    med = np.median((abs(Y_pred - Y_true) / Y_true) * 100)\n",
    "    return med\n",
    "\n",
    "def percentile_95(Y_true, Y_pred):\n",
    "    percentile = np.percentile((abs(Y_pred - Y_true) / Y_true) * 100, 95)\n",
    "    return percentile\n",
    "\n",
    "def percentile_90(Y_true, Y_pred):\n",
    "    percentile = np.percentile((abs(Y_pred - Y_true) / Y_true) * 100, 90)\n",
    "    return percentile\n",
    "\n",
    "properties = [\"polar_surface_area\", \"logS\", \"refractivity\", \"polarizability\", \"logP_alogps\", \"logP_chemaxon\"]\n",
    "for i, p in enumerate(properties):\n",
    "    print(p)\n",
    "    mse = mean_squared_error(y_true=Y_test[i], y_pred = y_pred[i])\n",
    "    r2 = r2_score(y_true=Y_test[i], y_pred = y_pred[i])\n",
    "    mean_abs_err = mean_absolute_error(y_true=Y_test[i], y_pred = y_pred[i])\n",
    "    median_abs_err = median_absolute_error(y_true=Y_test[i], y_pred = y_pred[i])\n",
    "    relative_mean_err = relative_mean(Y_test[i], y_pred[i].flatten())\n",
    "    relative_median_err = relative_median(Y_test[i], y_pred[i].flatten())\n",
    "    ninety = percentile_90(Y_test[i], y_pred[i].flatten())\n",
    "    ninety_five = percentile_95(Y_test[i], y_pred[i].flatten())\n",
    "    print(\"MSE: {}\".format(mse))\n",
    "    print(\"R2: {}\".format(r2))\n",
    "    print(\"median absolute: {}\".format(median_abs_err))\n",
    "    print(\"mean absolute: {}\".format(mean_abs_err))\n",
    "    print(\"median relative: {}\".format(relative_median_err))\n",
    "    print(\"mean relative: {}\".format(relative_mean_err))\n",
    "    print(\"90 %: {}\".format(ninety))\n",
    "    print(\"95 %: {}\".format(ninety_five))\n",
    "    print(\"------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the dataset in order to easily reproduce/reload the experiements\n",
    "import h5py as h5\n",
    "with h5.File(\"Multi-output_datasets.h5\", 'w') as fo:\n",
    "    fo.create_dataset(\"train_x\", data=X_train_encoded)\n",
    "    fo.create_dataset(\"train_y\", data=Y_train)\n",
    "    fo.create_dataset(\"valid_x\", data=X_valid_encoded)\n",
    "    fo.create_dataset(\"valid_y\", data=Y_valid)\n",
    "    fo.create_dataset(\"test_x\", data=X_test_encoded)\n",
    "    fo.create_dataset(\"test_y\", data=Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain model for CCS prediction\n",
    "Now that the multi-output model can perform predictions for multiple properties, we will use its internal representation to perform CCS prediction. This will tell us if we have enough data to train the internal representation for CCS predictions using only the CCS data. Furthermore, it will show that CNN are appropriate for molecular descriptor predictions directly from SMILES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the data exactly the same way as it was done for the first experiment\n",
    "from DeepCCS.utils import read_dataset, filter_data\n",
    "from DeepCCS.model.encoders import AdductToOneHotEncoder\n",
    "\n",
    "datafile = \"../DATASETS.h5\"\n",
    "datasets_names = [\"MetCCS_pos\", \"MetCCS_neg\", \"Agilent_pos\", \"Agilent_neg\", \"Waters_pos\", \"Waters_neg\",\n",
    "                 \"PNL\", \"McLean\", \"CBM\"] ## TODO: Change PNL to PNNL in the h5 datafile\n",
    "\n",
    "datasets = [read_dataset(datafile, d_name) for d_name in datasets_names] # Read\n",
    "datasets = [filter_data(d_set) for d_set in datasets] # Filter\n",
    "\n",
    "np.random.seed(777)\n",
    "save_test_sets_data = {}\n",
    "pooled_set = []\n",
    "test_sets = []\n",
    "train_set = []\n",
    "validation_set = []\n",
    "test_sets_names = []\n",
    "for i, dset in enumerate(datasets_names):\n",
    "    if dset in [\"MetCCS_pos\", \"MetCCS_neg\"]:\n",
    "        pooled_set.append([np.array(datasets[i][\"SMILES\"]),\n",
    "                           np.array(datasets[i][\"Adducts\"]),\n",
    "                           np.array(datasets[i][\"CCS\"])])\n",
    "    elif dset in [\"Agilent_pos\", \"Agilent_neg\", \"Waters_pos\", \"Waters_neg\"]:\n",
    "        test_sets.append([np.array(datasets[i][\"SMILES\"]),\n",
    "                          np.array(datasets[i][\"Adducts\"]),\n",
    "                          np.array(datasets[i][\"CCS\"])])\n",
    "        test_sets_names.append(dset)\n",
    "    elif dset in [\"PNL\", \"McLean\", \"CBM\"]:\n",
    "        smiles = np.array(datasets[i][\"SMILES\"])\n",
    "        ccs = np.array(datasets[i][\"CCS\"])\n",
    "        adducts = np.array(datasets[i][\"Adducts\"])\n",
    "        \n",
    "        # We use binary masks to split the datasets between pooled and test\n",
    "        mask_pooled = np.zeros(len(smiles), dtype=int)\n",
    "        mask_pooled[:int(len(smiles) * 0.8)] = 1  # The remaining 20% goes in the test set.\n",
    "        np.random.shuffle(mask_pooled)\n",
    "        mask_test = 1 - mask_pooled\n",
    "        mask_pooled = mask_pooled.astype(bool)\n",
    "        mask_test = mask_test.astype(bool)\n",
    "        \n",
    "        pooled_set.append([smiles[mask_pooled], adducts[mask_pooled], ccs[mask_pooled]])\n",
    "        test_sets.append([smiles[mask_test], adducts[mask_test], ccs[mask_test]])\n",
    "        test_sets_names.append(dset)\n",
    "# Split pooled between train (90%) and validation (10%)\n",
    "smiles_pooled = np.concatenate([i[0] for i in pooled_set])\n",
    "adducts_pooled = np.concatenate([i[1] for i in pooled_set])\n",
    "ccs_pooled = np.concatenate([i[2] for i in pooled_set])\n",
    "\n",
    "mask_train = np.zeros(len(smiles_pooled), dtype=int)\n",
    "mask_train[:int(len(smiles_pooled) * 0.9)] = 1  # The remaining 10% goes in the validation set.\n",
    "np.random.shuffle(mask_train)\n",
    "mask_valid = 1 - mask_train\n",
    "mask_train = mask_train.astype(bool)\n",
    "mask_valid = mask_valid.astype(bool)\n",
    "\n",
    "train_set = [smiles_pooled[mask_train], adducts_pooled[mask_train], ccs_pooled[mask_train]]\n",
    "validation_set = [smiles_pooled[mask_valid], adducts_pooled[mask_valid], ccs_pooled[mask_valid]]\n",
    "\n",
    "#We use the SMILES encoder that was used for encoding the HMDB database.\n",
    "train_set[0] = smiles_encoder_multi.transform(train_set[0])\n",
    "validation_set[0] = smiles_encoder_multi.transform(validation_set[0])\n",
    "\n",
    "adducts_encoder = AdductToOneHotEncoder()\n",
    "adducts_encoder.fit(np.concatenate([dset[\"Adducts\"] for dset in datasets]))\n",
    "train_set[1] = adducts_encoder.transform(train_set[1])\n",
    "validation_set[1] = adducts_encoder.transform(validation_set[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "smiles (InputLayer)             (None, 250, 58)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 247, 64)      14912       smiles[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 244, 64)      16448       conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 243, 64)      0           conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 240, 64)      16448       max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 239, 64)      0           conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 236, 64)      16448       max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, 235, 64)      0           conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 232, 64)      16448       max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 231, 64)      0           conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 228, 64)      16448       max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 227, 64)      0           conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 224, 64)      16448       max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 112, 64)      0           conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 7168)         0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "adduct (InputLayer)             (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 7172)         0           flatten_4[0][0]                  \n",
      "                                                                 adduct[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_58 (Dense)                (None, 384)          2754432     concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_59 (Dense)                (None, 384)          147840      dense_58[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_60 (Dense)                (None, 1)            385         dense_59[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 3,016,257\n",
      "Trainable params: 2,902,657\n",
      "Non-trainable params: 113,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Keep only the input layer and the conv. + max. pooling layers + the flatten layer\n",
    "del model.layers[15:]\n",
    "\n",
    "# Set trainable property at false to lock weights\n",
    "for l in model.layers:\n",
    "    l.trainable = False\n",
    "\n",
    "    \n",
    "adduct_input_layer = Input(shape=(len(adducts_encoder.converter),), name=\"adduct\")\n",
    "\n",
    "previous = model.layers[-1].output\n",
    "\n",
    "remix_layer = keras.layers.concatenate([previous, adduct_input_layer], axis=-1)\n",
    "previous = remix_layer\n",
    "\n",
    "# Insert new dense layers\n",
    "for i in range(2):\n",
    "    dense_layer = Dense(384, activation=\"relu\", kernel_initializer='normal')(previous)\n",
    "    previous = dense_layer\n",
    "\n",
    "#Insert new ouput layer\n",
    "output = Dense(1, activation=\"linear\")(previous)\n",
    "\n",
    "\n",
    "# Compile\n",
    "opt = adam(lr=0.0001)\n",
    "new_model = Model(inputs=[model.layers[0].input, adduct_input_layer], outputs=output)\n",
    "new_model.compile(optimizer=opt, loss='mean_squared_error')\n",
    "\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = \"Model_CCS_after_multioutput_20180912.h5\"\n",
    "model_checkpoint = ModelCheckpoint(model_file, save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the pure keras model to as DeepCCS model (done manually)\n",
    "from DeepCCS.model.DeepCCS import DeepCCSModel\n",
    "\n",
    "deepCCS_model = DeepCCSModel()\n",
    "deepCCS_model.adduct_encoder = adducts_encoder\n",
    "deepCCS_model.smiles_encoder = smiles_encoder_multi\n",
    "deepCCS_model.model = new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1665 samples, validate on 186 samples\n",
      "Epoch 1/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 6853.0228 - val_loss: 515.8792\n",
      "Epoch 2/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 469.9619 - val_loss: 377.9421\n",
      "Epoch 3/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 327.4491 - val_loss: 327.3235\n",
      "Epoch 4/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 221.0855 - val_loss: 194.8395\n",
      "Epoch 5/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 158.6047 - val_loss: 158.0601\n",
      "Epoch 6/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 120.1269 - val_loss: 146.2560\n",
      "Epoch 7/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 99.5317 - val_loss: 125.2390\n",
      "Epoch 8/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 85.1759 - val_loss: 115.0245\n",
      "Epoch 9/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 75.8128 - val_loss: 115.4641\n",
      "Epoch 10/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 66.9307 - val_loss: 109.0287\n",
      "Epoch 11/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 58.2423 - val_loss: 106.6528\n",
      "Epoch 12/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 57.6064 - val_loss: 105.1916\n",
      "Epoch 13/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 52.9674 - val_loss: 108.3318\n",
      "Epoch 14/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 48.4771 - val_loss: 99.3441\n",
      "Epoch 15/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 46.0864 - val_loss: 83.5719\n",
      "Epoch 16/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 45.4609 - val_loss: 92.0782\n",
      "Epoch 17/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 41.5934 - val_loss: 81.5597\n",
      "Epoch 18/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 43.0882 - val_loss: 100.8893\n",
      "Epoch 19/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 40.4472 - val_loss: 84.0362\n",
      "Epoch 20/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 37.0810 - val_loss: 87.0822\n",
      "Epoch 21/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 37.8856 - val_loss: 81.1835\n",
      "Epoch 22/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 34.3311 - val_loss: 80.5611\n",
      "Epoch 23/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 36.5115 - val_loss: 94.8991\n",
      "Epoch 24/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 36.1602 - val_loss: 79.7364\n",
      "Epoch 25/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 32.5698 - val_loss: 80.0714\n",
      "Epoch 26/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 32.6312 - val_loss: 80.4121\n",
      "Epoch 27/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 32.7405 - val_loss: 80.9878\n",
      "Epoch 28/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 32.6334 - val_loss: 92.9707\n",
      "Epoch 29/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 29.7587 - val_loss: 79.8220\n",
      "Epoch 30/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 27.9228 - val_loss: 75.6253\n",
      "Epoch 31/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 30.9893 - val_loss: 99.5644\n",
      "Epoch 32/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 26.5286 - val_loss: 77.5846\n",
      "Epoch 33/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 27.2015 - val_loss: 76.1014\n",
      "Epoch 34/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 27.5785 - val_loss: 74.6128\n",
      "Epoch 35/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 25.5253 - val_loss: 89.4756\n",
      "Epoch 36/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 26.5435 - val_loss: 71.4602\n",
      "Epoch 37/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 30.3243 - val_loss: 79.0908\n",
      "Epoch 38/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 25.7339 - val_loss: 73.9795\n",
      "Epoch 39/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 23.6190 - val_loss: 78.1560\n",
      "Epoch 40/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 24.4712 - val_loss: 73.4628\n",
      "Epoch 41/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 24.8358 - val_loss: 82.3618\n",
      "Epoch 42/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 25.5785 - val_loss: 74.2242\n",
      "Epoch 43/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 25.3598 - val_loss: 69.0632\n",
      "Epoch 44/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 23.1892 - val_loss: 70.4454\n",
      "Epoch 45/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 21.5328 - val_loss: 71.3674\n",
      "Epoch 46/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 23.8185 - val_loss: 78.4562\n",
      "Epoch 47/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 21.6162 - val_loss: 72.4532\n",
      "Epoch 48/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 21.0016 - val_loss: 74.7295\n",
      "Epoch 49/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 22.8074 - val_loss: 72.5793\n",
      "Epoch 50/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 21.5829 - val_loss: 77.8039\n",
      "Epoch 51/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 20.8909 - val_loss: 79.4755\n",
      "Epoch 52/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 22.3842 - val_loss: 77.1187\n",
      "Epoch 53/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 21.6291 - val_loss: 74.8241\n",
      "Epoch 54/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 23.7797 - val_loss: 78.0544\n",
      "Epoch 55/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 20.3583 - val_loss: 79.6461\n",
      "Epoch 56/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 19.0145 - val_loss: 77.5726\n",
      "Epoch 57/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 19.3425 - val_loss: 82.5301\n",
      "Epoch 58/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 19.2895 - val_loss: 99.1133\n",
      "Epoch 59/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 20.1508 - val_loss: 74.2652\n",
      "Epoch 60/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 20.3428 - val_loss: 71.5438\n",
      "Epoch 61/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 18.7313 - val_loss: 85.3097\n",
      "Epoch 62/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 17.1791 - val_loss: 78.9658\n",
      "Epoch 63/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 17.3412 - val_loss: 77.1669\n",
      "Epoch 64/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 20.8132 - val_loss: 91.8662\n",
      "Epoch 65/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 17.3507 - val_loss: 71.2802\n",
      "Epoch 66/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 19.5489 - val_loss: 72.6858\n",
      "Epoch 67/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 18.7687 - val_loss: 75.8576\n",
      "Epoch 68/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 16.8459 - val_loss: 77.1371\n",
      "Epoch 69/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 17.2515 - val_loss: 75.7043\n",
      "Epoch 70/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 18.9283 - val_loss: 85.0361\n",
      "Epoch 71/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 16.6000 - val_loss: 90.9734\n",
      "Epoch 72/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 18.0060 - val_loss: 77.8554\n",
      "Epoch 73/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 15.8403 - val_loss: 72.5069\n",
      "Epoch 74/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 16.5221 - val_loss: 83.5450\n",
      "Epoch 75/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 15.2412 - val_loss: 78.6249\n",
      "Epoch 76/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 16.6930 - val_loss: 87.6159\n",
      "Epoch 77/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 16.5340 - val_loss: 74.4619\n",
      "Epoch 78/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 16.1103 - val_loss: 78.9303\n",
      "Epoch 79/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 14.5926 - val_loss: 79.6809\n",
      "Epoch 80/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 16.0011 - val_loss: 80.5737\n",
      "Epoch 81/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 13.8059 - val_loss: 80.1846\n",
      "Epoch 82/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 16.1567 - val_loss: 75.5231\n",
      "Epoch 83/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 15.2832 - val_loss: 77.0957\n",
      "Epoch 84/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 15.9888 - val_loss: 79.7388\n",
      "Epoch 85/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 17.7720 - val_loss: 77.6745\n",
      "Epoch 86/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 13.1932 - val_loss: 73.8202\n",
      "Epoch 87/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 16.0010 - val_loss: 78.1169\n",
      "Epoch 88/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 13.9063 - val_loss: 78.6863\n",
      "Epoch 89/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 13.7096 - val_loss: 76.8629\n",
      "Epoch 90/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 14.3843 - val_loss: 88.5522\n",
      "Epoch 91/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 13.9212 - val_loss: 74.9836\n",
      "Epoch 92/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 14.3593 - val_loss: 82.8046\n",
      "Epoch 93/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 14.2287 - val_loss: 80.0798\n",
      "Epoch 94/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 13.0599 - val_loss: 88.1236\n",
      "Epoch 95/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 13.6365 - val_loss: 78.6498\n",
      "Epoch 96/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 11.6450 - val_loss: 82.9996\n",
      "Epoch 97/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 13.5959 - val_loss: 79.6642\n",
      "Epoch 98/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 12.0431 - val_loss: 79.8149\n",
      "Epoch 99/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 13.9455 - val_loss: 77.5413\n",
      "Epoch 100/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 12.2300 - val_loss: 76.4024\n",
      "Epoch 101/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 11.8995 - val_loss: 81.8159\n",
      "Epoch 102/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 13.6757 - val_loss: 76.6853\n",
      "Epoch 103/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 13.4956 - val_loss: 79.5271\n",
      "Epoch 104/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 11.2594 - val_loss: 89.8989\n",
      "Epoch 105/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 12.4527 - val_loss: 84.0174\n",
      "Epoch 106/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 11.7864 - val_loss: 81.2659\n",
      "Epoch 107/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 13.4168 - val_loss: 83.1727\n",
      "Epoch 108/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 14.5561 - val_loss: 82.0390\n",
      "Epoch 109/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 11.0731 - val_loss: 80.7305\n",
      "Epoch 110/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 11.3425 - val_loss: 84.0072\n",
      "Epoch 111/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 11.7660 - val_loss: 79.9795\n",
      "Epoch 112/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 11.9764 - val_loss: 86.6962\n",
      "Epoch 113/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 11.4076 - val_loss: 80.7869\n",
      "Epoch 114/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 10.5611 - val_loss: 76.1845\n",
      "Epoch 115/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 10.2826 - val_loss: 82.5960\n",
      "Epoch 116/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 11.2824 - val_loss: 73.5997\n",
      "Epoch 117/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 11.0893 - val_loss: 82.6316\n",
      "Epoch 118/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 10.5847 - val_loss: 77.6183\n",
      "Epoch 119/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 12.1622 - val_loss: 84.8825\n",
      "Epoch 120/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 10.1381 - val_loss: 80.4875\n",
      "Epoch 121/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 10.2740 - val_loss: 82.0836\n",
      "Epoch 122/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 10.3071 - val_loss: 81.0330\n",
      "Epoch 123/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 10.3911 - val_loss: 84.7361\n",
      "Epoch 124/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 10.5219 - val_loss: 76.8684\n",
      "Epoch 125/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 10.6476 - val_loss: 80.8725\n",
      "Epoch 126/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 11.1463 - val_loss: 89.2876\n",
      "Epoch 127/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 9.9877 - val_loss: 75.5945\n",
      "Epoch 128/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 10.4203 - val_loss: 82.7417\n",
      "Epoch 129/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 10.1387 - val_loss: 84.4203\n",
      "Epoch 130/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 10.3729 - val_loss: 88.7176\n",
      "Epoch 131/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 11.1621 - val_loss: 94.5291\n",
      "Epoch 132/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 9.8863 - val_loss: 86.1459\n",
      "Epoch 133/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 9.7798 - val_loss: 82.2900\n",
      "Epoch 134/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 10.0018 - val_loss: 93.6954\n",
      "Epoch 135/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 10.2596 - val_loss: 80.7741\n",
      "Epoch 136/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 9.2712 - val_loss: 81.7690\n",
      "Epoch 137/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 9.8322 - val_loss: 83.4139\n",
      "Epoch 138/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 8.7689 - val_loss: 78.8108\n",
      "Epoch 139/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 9.6879 - val_loss: 88.0270\n",
      "Epoch 140/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 9.3323 - val_loss: 78.7305\n",
      "Epoch 141/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 9.6214 - val_loss: 83.4045\n",
      "Epoch 142/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 9.9716 - val_loss: 78.8822\n",
      "Epoch 143/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 8.7589 - val_loss: 79.8410\n",
      "Epoch 144/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 8.1528 - val_loss: 81.4907\n",
      "Epoch 145/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 8.3278 - val_loss: 82.1137\n",
      "Epoch 146/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 8.6499 - val_loss: 81.0885\n",
      "Epoch 147/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 8.4580 - val_loss: 84.7622\n",
      "Epoch 148/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 8.9100 - val_loss: 82.8727\n",
      "Epoch 149/150\n",
      "1665/1665 [==============================] - 6s 4ms/step - loss: 9.2261 - val_loss: 103.9179\n",
      "Epoch 150/150\n",
      "1665/1665 [==============================] - 7s 4ms/step - loss: 9.2471 - val_loss: 82.9508\n"
     ]
    }
   ],
   "source": [
    "deepCCS_model.train_model(X1_train= train_set[0], X2_train=train_set[1], Y_train=train_set[2],\n",
    "                X1_valid=validation_set[0], X2_valid=validation_set[1], Y_valid=validation_set[2],\n",
    "                model_checkpoint=model_checkpoint, nbr_epochs=150, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a global test set (for overall performances)\n",
    "test_smiles_global = np.concatenate([t[0] for t in test_sets])\n",
    "test_adducts_global = np.concatenate([t[1] for t in test_sets])\n",
    "test_ccs_global = np.concatenate([t[2] for t in test_sets])\n",
    "test_sets.append([test_smiles_global, test_adducts_global, test_ccs_global])\n",
    "test_sets_names = test_sets_names + [\"global\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agilent_pos\n",
      "R2: 0.919076352719\n",
      "Mean relative: 2.89891866822\n",
      "Median relative: 2.37550877999\n",
      "-------------------------------------\n",
      "Agilent_neg\n",
      "R2: 0.941468724365\n",
      "Mean relative: 3.44637935935\n",
      "Median relative: 2.36446424801\n",
      "-------------------------------------\n",
      "Waters_pos\n",
      "R2: 0.862784391184\n",
      "Mean relative: 5.34968950804\n",
      "Median relative: 4.67779676596\n",
      "-------------------------------------\n",
      "Waters_neg\n",
      "R2: 0.937070612725\n",
      "Mean relative: 3.5584661639\n",
      "Median relative: 2.6294080517\n",
      "-------------------------------------\n",
      "PNL\n",
      "R2: 0.942251305733\n",
      "Mean relative: 3.7169720714\n",
      "Median relative: 2.82025336628\n",
      "-------------------------------------\n",
      "McLean\n",
      "R2: 0.985113610224\n",
      "Mean relative: 2.50557336567\n",
      "Median relative: 1.34630882518\n",
      "-------------------------------------\n",
      "CBM\n",
      "R2: 0.849334943527\n",
      "Mean relative: 3.9896841405\n",
      "Median relative: 3.5293720501\n",
      "-------------------------------------\n",
      "global\n",
      "R2: 0.962161974691\n",
      "Mean relative: 3.73751260529\n",
      "Median relative: 2.77720700728\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "deepCCS_model.model.load_weights(model_file)\n",
    "for i, t in enumerate(test_sets):\n",
    "    predictions = deepCCS_model.predict(t[0], t[1]).flatten()\n",
    "    print(test_sets_names[i])\n",
    "    print(\"R2: {}\".format(r2_score(y_true=t[2], y_pred=predictions)))\n",
    "    print(\"Mean relative: {}\".format(relative_mean(Y_true=t[2], Y_pred=predictions)))\n",
    "    print(\"Median relative: {}\".format(relative_median(Y_true=t[2], Y_pred=predictions)))\n",
    "    print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71232"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

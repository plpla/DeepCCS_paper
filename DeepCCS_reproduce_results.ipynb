{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepCCS\n",
    "This Notebook contains all the code needed to reproduce the results of section \"CCS prediction\" from the DeepCCS paper.\n",
    "It is also an example of how to use the DeepCCS python module. \n",
    "For most users, we recommand the use of the command line interface which is a lot easier to use if you don't have a prior knowledge of python and/or machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py as h5\n",
    "import datetime\n",
    "from DeepCCS.utils import *\n",
    "from DeepCCS.model.DeepCCS import DeepCCSModel\n",
    "from DeepCCS.model.encoders import AdductToOneHotEncoder, SmilesToOneHotEncoder\n",
    "from DeepCCS.model.splitter import SMILESsplitter\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, median_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot\n",
    "from plotly.offline import init_notebook_mode, iplot, plot\n",
    "from plotly import graph_objs as go\n",
    "init_notebook_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PATHs and variables\n",
    "DATE = datetime.datetime.now().strftime(\"%Y-%m%-d-\")\n",
    "datafile = \"DATASETS.h5\"\n",
    "models_path = \"ModelsAndTestSets/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data\n",
    "We first read the data from the hdf5 file supplied with DeepCCS. This file was created using data from different sources (see references in README) and isomeric SMILES were retrieved from PubChem using the web API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasets_names = [\"MetCCS_train_pos\", \"MetCCS_train_neg\", \"MetCCS_test_pos\", \"MetCCS_test_neg\", \n",
    "                  \"Astarita_pos\", \"Astarita_neg\",\"Baker\", \"McLean\", \"CBM\"] \n",
    "\n",
    "datasets = [read_dataset(datafile, d_name) for d_name in datasets_names] # Read\n",
    "datasets = [filter_data(d_set) for d_set in datasets] # Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration\n",
    "This step will help in removing unwanted data from the dataset and getting to know the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Look at SMILES length using a violin plot\n",
    "#Exceptionnaly uses seaborn because plotly does not provide an easy way to create a violin plot.\n",
    "\n",
    "#SMILES = [d[\"SMILES\"] for d in datasets]\n",
    "#smiles_splitter = SMILESsplitter()\n",
    "#l_split_smiles = []\n",
    "#sources = []\n",
    "#for j, d in enumerate(SMILES):\n",
    "#    l = [len(smiles_splitter.split(i)) for i in d]\n",
    "#    l_split_smiles += l\n",
    "#    sources += [datasets_names[j]] * len(l)\n",
    "    \n",
    "\n",
    "\n",
    "#sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "#sns.set_style(\"whitegrid\")\n",
    "#ax = sns.violinplot(y=l_split_smiles, x=sources)\n",
    "#sns.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n",
    "We will perform model training in 2 ways:\n",
    " * Single dataset split --> ten models\n",
    " * 10 different splits --> one model per split\n",
    "\n",
    "For both case, the datasets are splitted using the partitionning from the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single split\n",
    "We first split the datasets between training, validation and testing set. Then, we train 10 models for which the weight are initialized randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(777)\n",
    "save_test_sets_data = {}\n",
    "pooled_set = []\n",
    "test_sets = []\n",
    "train_set = []\n",
    "validation_set = []\n",
    "test_sets_names = []\n",
    "for i, dset in enumerate(datasets_names):\n",
    "    if dset in [\"MetCCS_train_pos\", \"MetCCS_train_neg\"]:\n",
    "        pooled_set.append([np.array(datasets[i][\"SMILES\"]),\n",
    "                           np.array(datasets[i][\"Adducts\"]),\n",
    "                           np.array(datasets[i][\"CCS\"])])\n",
    "    elif dset in [\"MetCCS_test_pos\", \"MetCCS_test_neg\", \"Astarita_pos\", \"Astarita_neg\"]:\n",
    "        test_sets.append([np.array(datasets[i][\"SMILES\"]),\n",
    "                          np.array(datasets[i][\"Adducts\"]),\n",
    "                          np.array(datasets[i][\"CCS\"])])\n",
    "        test_sets_names.append(dset)\n",
    "    elif dset in [\"Baker\", \"McLean\", \"CBM\"]:\n",
    "        smiles = np.array(datasets[i][\"SMILES\"])\n",
    "        ccs = np.array(datasets[i][\"CCS\"])\n",
    "        adducts = np.array(datasets[i][\"Adducts\"])\n",
    "        \n",
    "        # We use binary masks to split the datasets between pooled and test\n",
    "        mask_pooled = np.zeros(len(smiles), dtype=int)\n",
    "        mask_pooled[:int(len(smiles) * 0.8)] = 1  # The remaining 20% goes in the test set.\n",
    "        np.random.shuffle(mask_pooled)\n",
    "        mask_test = 1 - mask_pooled\n",
    "        mask_pooled = mask_pooled.astype(bool)\n",
    "        mask_test = mask_test.astype(bool)\n",
    "        \n",
    "        pooled_set.append([smiles[mask_pooled], adducts[mask_pooled], ccs[mask_pooled]])\n",
    "        test_sets.append([smiles[mask_test], adducts[mask_test], ccs[mask_test]])\n",
    "        test_sets_names.append(dset)\n",
    "# Split pooled between train (90%) and validation (10%)\n",
    "smiles_pooled = np.concatenate([i[0] for i in pooled_set])\n",
    "adducts_pooled = np.concatenate([i[1] for i in pooled_set])\n",
    "ccs_pooled = np.concatenate([i[2] for i in pooled_set])\n",
    "\n",
    "mask_train = np.zeros(len(smiles_pooled), dtype=int)\n",
    "mask_train[:int(len(smiles_pooled) * 0.9)] = 1  # The remaining 10% goes in the validation set.\n",
    "np.random.shuffle(mask_train)\n",
    "mask_valid = 1 - mask_train\n",
    "mask_train = mask_train.astype(bool)\n",
    "mask_valid = mask_valid.astype(bool)\n",
    "\n",
    "train_set = [smiles_pooled[mask_train], adducts_pooled[mask_train], ccs_pooled[mask_train]]\n",
    "validation_set = [smiles_pooled[mask_valid], adducts_pooled[mask_valid], ccs_pooled[mask_valid]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test sets still contain each test dataset individually to easily calculate metrics independently.\n",
    "\n",
    "Now, we will encode the SMILES and adducts in order to be able to feed them to the network. We will use one-hot vector encoding. We will use the encoders already implemented in DeepCCS. The main advantage of using DeepCCS encoders is that the SMILES encoder automatically pad the smiles to a specified length and that they offer  load/save functionnalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smiles_encoder = SmilesToOneHotEncoder()\n",
    "smiles_encoder.fit(np.concatenate([dset[\"SMILES\"] for dset in datasets]))\n",
    "train_set[0] = smiles_encoder.transform(train_set[0])\n",
    "validation_set[0] = smiles_encoder.transform(validation_set[0])\n",
    "\n",
    "adducts_encoder = AdductToOneHotEncoder()\n",
    "adducts_encoder.fit(np.concatenate([dset[\"Adducts\"] for dset in datasets]))\n",
    "train_set[1] = adducts_encoder.transform(train_set[1])\n",
    "validation_set[1] = adducts_encoder.transform(validation_set[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smiles_encoder.save_encoder(\"SMILES_encoder.json\")\n",
    "adducts_encoder.save_encoder(\"Adducts_encoder.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the neural network\n",
    "The data is ready. We can now create our model and train it. Since the results are dependant of the initialisation, we will retrain the network 10 times in order to have a good idea of the reproducibility. Each model will be saved in a file so that they can be reloaded.\n",
    "\n",
    "Let's just look at the network structure before we go further..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "smile (InputLayer)              (None, 250, 37)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 247, 64)      9536        smile[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 244, 64)      16448       conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 243, 64)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 240, 64)      16448       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 239, 64)      0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 236, 64)      16448       max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 235, 64)      0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 232, 64)      16448       max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 231, 64)      0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 228, 64)      16448       max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 227, 64)      0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 224, 64)      16448       max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 112, 64)      0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 7168)         0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "adduct (InputLayer)             (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 7172)         0           flatten_1[0][0]                  \n",
      "                                                                 adduct[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 384)          2754432     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 384)          147840      dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            385         dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,010,881\n",
      "Trainable params: 3,010,881\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elinaff/anaconda2/lib/python2.7/site-packages/DeepCCS-0.0.1-py2.7.egg/DeepCCS/model/DeepCCS.py:97: UserWarning:\n",
      "\n",
      "Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=[<tf.Tenso...)`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = DeepCCSModel()\n",
    "model.adduct_encoder = adducts_encoder\n",
    "model.smiles_encoder = smiles_encoder\n",
    "model.create_model()\n",
    "model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save test set to file for furter use\n",
    "f_out = h5.File(models_path+DATE+\"SingleSplit_TestSets.h5\", 'w')\n",
    "dt = h5.special_dtype(vlen=str)\n",
    "for j, name in enumerate(test_sets_names):\n",
    "    f_out.create_dataset(name, data=np.array(test_sets[j]), dtype=dt)\n",
    "f_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This can take a while... We will only train the networks in a first loop.\n",
    "# We will evaluate the models once the training is completed\n",
    "for i in range(10):\n",
    "    # Model training\n",
    "    model = DeepCCSModel()\n",
    "    model.adduct_encoder = adducts_encoder\n",
    "    model.smiles_encoder = smiles_encoder\n",
    "    model.create_model()\n",
    "    m_checkpoint = ModelCheckpoint(models_path+DATE+\"SingleSplit_ModelWeigth_\"+str(i)+\".h5\", save_best_only=True, save_weights_only=True)\n",
    "    model.train_model(X1_train= train_set[0], X2_train=train_set[1], Y_train=train_set[2],\n",
    "                X1_valid=validation_set[0], X2_valid=validation_set[1], Y_valid=validation_set[2],\n",
    "                model_checkpoint=m_checkpoint, nbr_epochs=150, verbose=1)\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-split\n",
    "We re-split the datasets randomly before training each model. This will allow to measure the impact of dataset splitting on the model performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(666)\n",
    "r2 = []\n",
    "mean_relative = []\n",
    "median_relative = []\n",
    "\n",
    "datasets = [read_dataset(datafile, d_name) for d_name in datasets_names] # Read\n",
    "datasets = [filter_data(d_set) for d_set in datasets] # Filter\n",
    "\n",
    "for xi in range(10):\n",
    "    pooled_set = []\n",
    "    test_sets = []\n",
    "    train_set = []\n",
    "    validation_set = []\n",
    "    test_sets_names = []\n",
    "    for i, dset in enumerate(datasets_names):\n",
    "        if dset in [\"MetCCS_train_pos\", \"MetCCS_train_neg\"]:\n",
    "            pooled_set.append([np.array(datasets[i][\"SMILES\"]),\n",
    "                               np.array(datasets[i][\"Adducts\"]),\n",
    "                               np.array(datasets[i][\"CCS\"])])\n",
    "        elif dset in [\"MetCCS_test_pos\", \"MetCCS_test_neg\", \"Astarita_pos\", \"Astarita_neg\"]:\n",
    "            test_sets.append([np.array(datasets[i][\"SMILES\"]),\n",
    "                              np.array(datasets[i][\"Adducts\"]),\n",
    "                              np.array(datasets[i][\"CCS\"])])\n",
    "            test_sets_names.append(dset)\n",
    "        elif dset in [\"Baker\", \"McLean\", \"CBM\"]:\n",
    "            smiles = np.array(datasets[i][\"SMILES\"])\n",
    "            ccs = np.array(datasets[i][\"CCS\"])\n",
    "            adducts = np.array(datasets[i][\"Adducts\"])\n",
    "\n",
    "            # We use binary masks to split the datasets between pooled and test\n",
    "            mask_pooled = np.zeros(len(smiles), dtype=int)\n",
    "            mask_pooled[:int(len(smiles) * 0.8)] = 1  # The remaining 20% goes in the test set.\n",
    "            np.random.shuffle(mask_pooled)\n",
    "            mask_test = 1 - mask_pooled\n",
    "            mask_pooled = mask_pooled.astype(bool)\n",
    "            mask_test = mask_test.astype(bool)\n",
    "\n",
    "            pooled_set.append([smiles[mask_pooled], adducts[mask_pooled], ccs[mask_pooled]])\n",
    "            test_sets.append([smiles[mask_test], adducts[mask_test], ccs[mask_test]])\n",
    "            test_sets_names.append(dset)\n",
    "    # Split pooled between train (90%) and validation (10%)\n",
    "    smiles_pooled = np.concatenate([i[0] for i in pooled_set])\n",
    "    adducts_pooled = np.concatenate([i[1] for i in pooled_set])\n",
    "    ccs_pooled = np.concatenate([i[2] for i in pooled_set])\n",
    "\n",
    "    mask_train = np.zeros(len(smiles_pooled), dtype=int)\n",
    "    mask_train[:int(len(smiles_pooled) * 0.9)] = 1  # The remaining 20% goes in the test set.\n",
    "    np.random.shuffle(mask_train)\n",
    "    mask_valid = 1 - mask_train\n",
    "    mask_train = mask_train.astype(bool)\n",
    "    mask_valid = mask_valid.astype(bool)\n",
    "\n",
    "    train_set = [smiles_pooled[mask_train], adducts_pooled[mask_train], ccs_pooled[mask_train]]\n",
    "    validation_set = [smiles_pooled[mask_valid], adducts_pooled[mask_valid], ccs_pooled[mask_valid]]\n",
    "\n",
    "    # One hot vector Encoding\n",
    "    train_set[0] = smiles_encoder.transform(train_set[0])\n",
    "    validation_set[0] = smiles_encoder.transform(validation_set[0])\n",
    "    \n",
    "    train_set[1] = adducts_encoder.transform(train_set[1])\n",
    "    validation_set[1] = adducts_encoder.transform(validation_set[1])\n",
    "\n",
    "    #Model training\n",
    "    model = DeepCCSModel()\n",
    "    model.adduct_encoder = adducts_encoder\n",
    "    model.smiles_encoder = smiles_encoder\n",
    "    model.create_model()\n",
    "    m_checkpoint = ModelCheckpoint(models_path+DATE+\"MultiSplit_ModelWeigth_\"+str(xi)+\".h5\", save_best_only=True, save_weights_only=True)\n",
    "    model.train_model(X1_train= train_set[0], X2_train=train_set[1], Y_train=train_set[2],\n",
    "                X1_valid=validation_set[0], X2_valid=validation_set[1], Y_valid=validation_set[2],\n",
    "                model_checkpoint=m_checkpoint, nbr_epochs=150, verbose=1)\n",
    "    \n",
    "    #Save test data in case kernel dies.\n",
    "    f_out = h5.File(models_path+DATE+\"MultiSplit_\" + str(xi) + \"_TestSet.h5\", 'w')\n",
    "    dt = h5.special_dtype(vlen=str)\n",
    "    for j, name in enumerate(test_sets_names):\n",
    "        f_out.create_dataset(name, data=np.array(test_sets[j]), dtype=dt)\n",
    "    f_out.close()\n",
    "    K.clear_session()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the models performances\n",
    "## Single split experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agilent_neg\n",
      "57\n",
      "Agilent_pos\n",
      "74\n",
      "CBM\n",
      "72\n",
      "McLean\n",
      "52\n",
      "PNL\n",
      "172\n",
      "Waters_neg\n",
      "113\n",
      "Waters_pos\n",
      "92\n",
      "There are 8 test sets.\n",
      "They are: [u'Agilent_neg', u'Agilent_pos', u'CBM', u'McLean', u'PNL', u'Waters_neg', u'Waters_pos', 'global']\n"
     ]
    }
   ],
   "source": [
    "#Load single split results\n",
    "\n",
    "test_sets_file = h5.File(models_path+DATE+\"SingleSplit_TestSets.h5\", 'r')\n",
    "test_sets_names = []\n",
    "test_sets = []\n",
    "for dset in test_sets_file:\n",
    "    test_sets_names.append(dset)\n",
    "    print(dset)\n",
    "    test_sets.append(np.array(test_sets_file[dset]))\n",
    "    print(len(np.array(test_sets_file[dset])[0]))\n",
    "\n",
    "#Convert back CCS from string to float\n",
    "for i,j in enumerate(test_sets):\n",
    "    test_sets[i][2] = np.array(test_sets[i][2], dtype=float)\n",
    "    \n",
    "#Create a global test set that contains everything.\n",
    "test_smiles_global = np.concatenate([t[0] for t in test_sets])\n",
    "test_adducts_global = np.concatenate([t[1] for t in test_sets])\n",
    "test_ccs_global = np.concatenate([t[2] for t in test_sets])\n",
    "test_sets.append([test_smiles_global, test_adducts_global, test_ccs_global])\n",
    "test_sets_names = test_sets_names + [\"global\"]\n",
    "\n",
    "print(\"There are {} test sets.\".format(len(test_sets)))\n",
    "print(\"They are: {}\".format(test_sets_names))\n",
    "\n",
    "ss_r2 = []\n",
    "ss_mean_relative = []\n",
    "ss_median_relative = []\n",
    "\n",
    "adducts_encoder = AdductToOneHotEncoder()\n",
    "adducts_encoder.load_encoder(\"Adducts_encoder.json\")\n",
    "smiles_encoder = SmilesToOneHotEncoder()\n",
    "smiles_encoder.load_encoder(\"SMILES_encoder.json\")\n",
    "\n",
    "for i in range(10):\n",
    "    model = DeepCCSModel()\n",
    "    model.adduct_encoder = adducts_encoder\n",
    "    model.smiles_encoder = smiles_encoder\n",
    "    model.create_model()\n",
    "    model.model.load_weights(models_path+DATE+\"SingleSplit_ModelWeigth_\"+str(i)+\".h5\")\n",
    "    model._is_fit = True\n",
    "    for t in test_sets:\n",
    "        predictions = model.predict(t[0], t[1]).flatten()\n",
    "        ss_r2.append(r2_score(y_true=t[2], y_pred=predictions))\n",
    "        ss_mean_relative.append(relative_mean(Y_true=t[2], Y_pred=predictions))\n",
    "        ss_median_relative.append(relative_median(Y_true=t[2], Y_pred=predictions))\n",
    "\n",
    "#Reshape for table creation        \n",
    "ss_r2 = np.reshape(ss_r2, (10,8))\n",
    "ss_mean_relative = np.reshape(ss_mean_relative, (10,8))\n",
    "ss_median_relative = np.reshape(ss_median_relative, (10,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "cells": {
          "values": [
           [
            "R2",
            "median_relative_error",
            "mean_relative_error"
           ],
           [
            0.969,
            3.108,
            3.204
           ],
           [
            0.96,
            2.02,
            2.372
           ],
           [
            0.93,
            2.26,
            2.656
           ],
           [
            0.995,
            1.493,
            1.924
           ],
           [
            0.954,
            2.431,
            3.197
           ],
           [
            0.955,
            3.138,
            3.605
           ],
           [
            0.901,
            4.866,
            4.88
           ],
           [
            0.976,
            2.665,
            3.253
           ]
          ]
         },
         "header": {
          "values": [
           "",
           "Agilent_neg",
           "Agilent_pos",
           "CBM",
           "McLean",
           "PNL",
           "Waters_neg",
           "Waters_pos",
           "global"
          ]
         },
         "type": "table",
         "uid": "45aa0210-03ab-11e9-bc9a-705a0f47fdfe"
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div id=\"c958322b-0fcf-4031-ad5e-6090525feab5\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"c958322b-0fcf-4031-ad5e-6090525feab5\", [{\"header\": {\"values\": [\"\", \"Agilent_neg\", \"Agilent_pos\", \"CBM\", \"McLean\", \"PNL\", \"Waters_neg\", \"Waters_pos\", \"global\"]}, \"cells\": {\"values\": [[\"R2\", \"median_relative_error\", \"mean_relative_error\"], [0.969, 3.108, 3.204], [0.96, 2.02, 2.372], [0.93, 2.26, 2.656], [0.995, 1.493, 1.924], [0.954, 2.431, 3.197], [0.955, 3.138, 3.605], [0.901, 4.866, 4.88], [0.976, 2.665, 3.253]]}, \"type\": \"table\", \"uid\": \"45aa0211-03ab-11e9-bc9a-705a0f47fdfe\"}], {}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"c958322b-0fcf-4031-ad5e-6090525feab5\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"c958322b-0fcf-4031-ad5e-6090525feab5\", [{\"header\": {\"values\": [\"\", \"Agilent_neg\", \"Agilent_pos\", \"CBM\", \"McLean\", \"PNL\", \"Waters_neg\", \"Waters_pos\", \"global\"]}, \"cells\": {\"values\": [[\"R2\", \"median_relative_error\", \"mean_relative_error\"], [0.969, 3.108, 3.204], [0.96, 2.02, 2.372], [0.93, 2.26, 2.656], [0.995, 1.493, 1.924], [0.954, 2.431, 3.197], [0.955, 3.138, 3.605], [0.901, 4.866, 4.88], [0.976, 2.665, 3.253]]}, \"type\": \"table\", \"uid\": \"45aa0211-03ab-11e9-bc9a-705a0f47fdfe\"}], {}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "cells": {
          "values": [
           [
            "R2",
            "median_relative_error",
            "mean_relative_error"
           ],
           [
            0.005,
            0.487,
            0.322
           ],
           [
            0.005,
            0.239,
            0.187
           ],
           [
            0.01,
            0.282,
            0.193
           ],
           [
            0.001,
            0.138,
            0.086
           ],
           [
            0.006,
            0.114,
            0.058
           ],
           [
            0.006,
            0.483,
            0.345
           ],
           [
            0.013,
            0.295,
            0.367
           ],
           [
            0.001,
            0.183,
            0.158
           ]
          ]
         },
         "header": {
          "values": [
           "",
           "Agilent_neg",
           "Agilent_pos",
           "CBM",
           "McLean",
           "PNL",
           "Waters_neg",
           "Waters_pos",
           "global"
          ]
         },
         "type": "table",
         "uid": "45aa0212-03ab-11e9-bc9a-705a0f47fdfe"
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div id=\"e21a42b3-9e06-4f1c-b6c7-a93ee7f9afa2\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"e21a42b3-9e06-4f1c-b6c7-a93ee7f9afa2\", [{\"header\": {\"values\": [\"\", \"Agilent_neg\", \"Agilent_pos\", \"CBM\", \"McLean\", \"PNL\", \"Waters_neg\", \"Waters_pos\", \"global\"]}, \"cells\": {\"values\": [[\"R2\", \"median_relative_error\", \"mean_relative_error\"], [0.005, 0.487, 0.322], [0.005, 0.239, 0.187], [0.01, 0.282, 0.193], [0.001, 0.138, 0.086], [0.006, 0.114, 0.058], [0.006, 0.483, 0.345], [0.013, 0.295, 0.367], [0.001, 0.183, 0.158]]}, \"type\": \"table\", \"uid\": \"45aa0213-03ab-11e9-bc9a-705a0f47fdfe\"}], {}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"e21a42b3-9e06-4f1c-b6c7-a93ee7f9afa2\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"e21a42b3-9e06-4f1c-b6c7-a93ee7f9afa2\", [{\"header\": {\"values\": [\"\", \"Agilent_neg\", \"Agilent_pos\", \"CBM\", \"McLean\", \"PNL\", \"Waters_neg\", \"Waters_pos\", \"global\"]}, \"cells\": {\"values\": [[\"R2\", \"median_relative_error\", \"mean_relative_error\"], [0.005, 0.487, 0.322], [0.005, 0.239, 0.187], [0.01, 0.282, 0.193], [0.001, 0.138, 0.086], [0.006, 0.114, 0.058], [0.006, 0.483, 0.345], [0.013, 0.295, 0.367], [0.001, 0.183, 0.158]]}, \"type\": \"table\", \"uid\": \"45aa0213-03ab-11e9-bc9a-705a0f47fdfe\"}], {}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Results table\n",
    "test_sets_names = [\"\"] + test_sets_names\n",
    "metrics = [ss_r2,  ss_median_relative, ss_mean_relative]\n",
    "averages = [[\"R2\", \"median_relative_error\", \"mean_relative_error\"]]\n",
    "stds = [[\"R2\", \"median_relative_error\", \"mean_relative_error\"]]\n",
    "for i, dset in enumerate(test_sets):\n",
    "    t_averages = []\n",
    "    t_stds = []\n",
    "    for metric in metrics:\n",
    "        t_averages.append(np.round(np.mean([d[i] for d in metric]), decimals=3))\n",
    "        t_stds.append(np.round(np.std([d[i] for d in metric]), decimals=3))\n",
    "    averages.append(t_averages)\n",
    "    stds.append(t_stds)\n",
    "    \n",
    "table_average = go.Table(\n",
    "    header=dict(values=test_sets_names),\n",
    "    cells=dict(values=averages))\n",
    "\n",
    "table_stds = go.Table(\n",
    "    header=dict(values=test_sets_names),\n",
    "    cells=dict(values=stds))\n",
    "\n",
    "iplot([table_average])\n",
    "iplot([table_stds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', u'Agilent_neg', u'Agilent_pos', u'CBM', u'McLean', u'PNL', u'Waters_neg', u'Waters_pos', 'global']\n",
      "[ 0.97778957  0.96852226  0.93652493  0.9940649   0.94818697  0.9659747\n",
      "  0.9266496   0.97705955]\n",
      "[ 2.30536868  1.63401977  2.21375592  1.77327552  2.52938721  2.21003961\n",
      "  4.22518478  2.28376343]\n"
     ]
    }
   ],
   "source": [
    "print(test_sets_names)\n",
    "print(ss_r2[1])\n",
    "print(ss_median_relative[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "cells": {
          "values": [
           [
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10
           ],
           [
            0.977,
            0.977,
            0.975,
            0.975,
            0.973,
            0.975,
            0.976,
            0.977,
            0.975,
            0.976
           ],
           [
            2.852,
            2.284,
            2.745,
            2.666,
            2.835,
            2.543,
            2.813,
            2.493,
            2.556,
            2.865
           ],
           [
            3.321,
            2.974,
            3.309,
            3.255,
            3.503,
            3.181,
            3.382,
            3.045,
            3.145,
            3.413
           ]
          ]
         },
         "header": {
          "values": [
           "Model",
           "R2",
           "Median relative",
           "mean relative"
          ]
         },
         "type": "table",
         "uid": "564a316c-03ab-11e9-bc9a-705a0f47fdfe"
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div id=\"84a09e40-308e-4214-bcba-aa9d7cc5a296\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"84a09e40-308e-4214-bcba-aa9d7cc5a296\", [{\"header\": {\"values\": [\"Model\", \"R2\", \"Median relative\", \"mean relative\"]}, \"cells\": {\"values\": [[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0], [0.977, 0.977, 0.975, 0.975, 0.973, 0.975, 0.976, 0.977, 0.975, 0.976], [2.852, 2.284, 2.745, 2.666, 2.835, 2.543, 2.813, 2.493, 2.556, 2.865], [3.321, 2.974, 3.309, 3.255, 3.503, 3.181, 3.382, 3.045, 3.145, 3.413]]}, \"type\": \"table\", \"uid\": \"564a316d-03ab-11e9-bc9a-705a0f47fdfe\"}], {}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"84a09e40-308e-4214-bcba-aa9d7cc5a296\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"84a09e40-308e-4214-bcba-aa9d7cc5a296\", [{\"header\": {\"values\": [\"Model\", \"R2\", \"Median relative\", \"mean relative\"]}, \"cells\": {\"values\": [[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0], [0.977, 0.977, 0.975, 0.975, 0.973, 0.975, 0.976, 0.977, 0.975, 0.976], [2.852, 2.284, 2.745, 2.666, 2.835, 2.543, 2.813, 2.493, 2.556, 2.865], [3.321, 2.974, 3.309, 3.255, 3.503, 3.181, 3.382, 3.045, 3.145, 3.413]]}, \"type\": \"table\", \"uid\": \"564a316d-03ab-11e9-bc9a-705a0f47fdfe\"}], {}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Extract results of the first model..\n",
    "\n",
    "metrics = [ss_r2,  ss_median_relative, ss_mean_relative]\n",
    "\n",
    "values_to_table = [[1,2,3,4,5,6,7,8,9,10],\n",
    "                   [m[-1] for m in ss_r2], \n",
    "                   [m[-1] for m in ss_median_relative], \n",
    "                   [m[-1] for m in ss_mean_relative]]\n",
    "\n",
    "table_search_best_model = go.Table(\n",
    "    header=dict(values=[\"Model\",\"R2\", \"Median relative\", \"mean relative\"]),\n",
    "    cells=dict(values=np.round(values_to_table, decimals=3)))\n",
    "\n",
    "iplot([table_search_best_model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-split results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 test sets.\n",
      "They are: [u'Agilent_neg', u'Agilent_pos', u'CBM', u'McLean', u'PNL', u'Waters_neg', u'Waters_pos', 'global']\n",
      "There are 8 test sets.\n",
      "They are: [u'Agilent_neg', u'Agilent_pos', u'CBM', u'McLean', u'PNL', u'Waters_neg', u'Waters_pos', 'global']\n",
      "There are 8 test sets.\n",
      "They are: [u'Agilent_neg', u'Agilent_pos', u'CBM', u'McLean', u'PNL', u'Waters_neg', u'Waters_pos', 'global']\n",
      "There are 8 test sets.\n",
      "They are: [u'Agilent_neg', u'Agilent_pos', u'CBM', u'McLean', u'PNL', u'Waters_neg', u'Waters_pos', 'global']\n",
      "There are 8 test sets.\n",
      "They are: [u'Agilent_neg', u'Agilent_pos', u'CBM', u'McLean', u'PNL', u'Waters_neg', u'Waters_pos', 'global']\n",
      "There are 8 test sets.\n",
      "They are: [u'Agilent_neg', u'Agilent_pos', u'CBM', u'McLean', u'PNL', u'Waters_neg', u'Waters_pos', 'global']\n",
      "There are 8 test sets.\n",
      "They are: [u'Agilent_neg', u'Agilent_pos', u'CBM', u'McLean', u'PNL', u'Waters_neg', u'Waters_pos', 'global']\n",
      "There are 8 test sets.\n",
      "They are: [u'Agilent_neg', u'Agilent_pos', u'CBM', u'McLean', u'PNL', u'Waters_neg', u'Waters_pos', 'global']\n",
      "There are 8 test sets.\n",
      "They are: [u'Agilent_neg', u'Agilent_pos', u'CBM', u'McLean', u'PNL', u'Waters_neg', u'Waters_pos', 'global']\n",
      "There are 8 test sets.\n",
      "They are: [u'Agilent_neg', u'Agilent_pos', u'CBM', u'McLean', u'PNL', u'Waters_neg', u'Waters_pos', 'global']\n"
     ]
    }
   ],
   "source": [
    "#Load multi-split results\n",
    "ms_r2 = []\n",
    "ms_mean_relative = []\n",
    "ms_median_relative = []\n",
    "\n",
    "for xi in range(10):\n",
    "    model = DeepCCSModel()\n",
    "    model.adduct_encoder = adducts_encoder\n",
    "    model.smiles_encoder = smiles_encoder\n",
    "    model.create_model()\n",
    "    model.model.load_weights(models_path+DATE+\"MultiSplit_ModelWeigth_\"+str(xi)+\".h5\")\n",
    "    model._is_fit = True\n",
    "    \n",
    "    test_set_file = h5.File(models_path+DATE+\"MultiSplit_\"+str(xi)+\"_TestSet.h5\", 'r')\n",
    "    test_sets_names = []\n",
    "    test_sets = []\n",
    "    for dset in test_sets_file:\n",
    "        test_sets_names.append(dset)\n",
    "        test_sets.append(np.array(test_sets_file[dset]))\n",
    "\n",
    "    #Convert back CCS from string to float\n",
    "    for i,j in enumerate(test_sets):\n",
    "        test_sets[i][2] = np.array(test_sets[i][2], dtype=float)\n",
    "\n",
    "    #Create a global test set that contains everything.\n",
    "    test_smiles_global = np.concatenate([t[0] for t in test_sets])\n",
    "    test_adducts_global = np.concatenate([t[1] for t in test_sets])\n",
    "    test_ccs_global = np.concatenate([t[2] for t in test_sets])\n",
    "    test_sets.append([test_smiles_global, test_adducts_global, test_ccs_global])\n",
    "    test_sets_names = test_sets_names + [\"global\"]\n",
    "    \n",
    "    print(\"There are {} test sets.\".format(len(test_sets)))\n",
    "    print(\"They are: {}\".format(test_sets_names))\n",
    "    \n",
    "    for t in test_sets:\n",
    "        predictions = model.predict(t[0], t[1]).flatten()\n",
    "        ms_r2.append(r2_score(y_true=t[2], y_pred=predictions))\n",
    "        ms_mean_relative.append(relative_mean(Y_true=t[2], Y_pred=predictions))\n",
    "        ms_median_relative.append(relative_median(Y_true=t[2], Y_pred=predictions))\n",
    "        \n",
    "ms_r2 = np.reshape(ms_r2, (10,8))\n",
    "ms_mean_relative = np.reshape(ms_mean_relative, (10,8))\n",
    "ms_median_relative = np.reshape(ms_median_relative, (10,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "cells": {
          "values": [
           [
            "R2",
            "median_relative_error",
            "mean_relative_error"
           ],
           [
            0.967,
            3.149,
            3.321
           ],
           [
            0.964,
            1.933,
            2.275
           ],
           [
            0.969,
            1.26,
            1.656
           ],
           [
            0.996,
            1.151,
            1.538
           ],
           [
            0.967,
            2.023,
            2.724
           ],
           [
            0.949,
            3.36,
            3.8
           ],
           [
            0.897,
            4.769,
            4.948
           ],
           [
            0.979,
            2.369,
            3.022
           ]
          ]
         },
         "header": {
          "values": [
           "",
           "Agilent_neg",
           "Agilent_pos",
           "CBM",
           "McLean",
           "PNL",
           "Waters_neg",
           "Waters_pos",
           "global"
          ]
         },
         "type": "table",
         "uid": "7f847d9e-03ab-11e9-bc9a-705a0f47fdfe"
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div id=\"6baf4a11-e45a-4df8-b7c0-03c1e76d5db7\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"6baf4a11-e45a-4df8-b7c0-03c1e76d5db7\", [{\"header\": {\"values\": [\"\", \"Agilent_neg\", \"Agilent_pos\", \"CBM\", \"McLean\", \"PNL\", \"Waters_neg\", \"Waters_pos\", \"global\"]}, \"cells\": {\"values\": [[\"R2\", \"median_relative_error\", \"mean_relative_error\"], [0.967, 3.149, 3.321], [0.964, 1.933, 2.275], [0.969, 1.26, 1.656], [0.996, 1.151, 1.538], [0.967, 2.023, 2.724], [0.949, 3.36, 3.8], [0.897, 4.769, 4.948], [0.979, 2.369, 3.022]]}, \"type\": \"table\", \"uid\": \"7f847d9f-03ab-11e9-bc9a-705a0f47fdfe\"}], {}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"6baf4a11-e45a-4df8-b7c0-03c1e76d5db7\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"6baf4a11-e45a-4df8-b7c0-03c1e76d5db7\", [{\"header\": {\"values\": [\"\", \"Agilent_neg\", \"Agilent_pos\", \"CBM\", \"McLean\", \"PNL\", \"Waters_neg\", \"Waters_pos\", \"global\"]}, \"cells\": {\"values\": [[\"R2\", \"median_relative_error\", \"mean_relative_error\"], [0.967, 3.149, 3.321], [0.964, 1.933, 2.275], [0.969, 1.26, 1.656], [0.996, 1.151, 1.538], [0.967, 2.023, 2.724], [0.949, 3.36, 3.8], [0.897, 4.769, 4.948], [0.979, 2.369, 3.022]]}, \"type\": \"table\", \"uid\": \"7f847d9f-03ab-11e9-bc9a-705a0f47fdfe\"}], {}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "cells": {
          "values": [
           [
            "R2",
            "median_relative_error",
            "mean_relative_error"
           ],
           [
            0.007,
            0.625,
            0.426
           ],
           [
            0.007,
            0.346,
            0.265
           ],
           [
            0.01,
            0.469,
            0.335
           ],
           [
            0.001,
            0.284,
            0.283
           ],
           [
            0.01,
            0.177,
            0.169
           ],
           [
            0.008,
            0.369,
            0.301
           ],
           [
            0.011,
            0.443,
            0.317
           ],
           [
            0.004,
            0.274,
            0.238
           ]
          ]
         },
         "header": {
          "values": [
           "",
           "Agilent_neg",
           "Agilent_pos",
           "CBM",
           "McLean",
           "PNL",
           "Waters_neg",
           "Waters_pos",
           "global"
          ]
         },
         "type": "table",
         "uid": "7f847da0-03ab-11e9-bc9a-705a0f47fdfe"
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div id=\"6fd76071-b023-4c22-8797-610d38cacd15\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"6fd76071-b023-4c22-8797-610d38cacd15\", [{\"header\": {\"values\": [\"\", \"Agilent_neg\", \"Agilent_pos\", \"CBM\", \"McLean\", \"PNL\", \"Waters_neg\", \"Waters_pos\", \"global\"]}, \"cells\": {\"values\": [[\"R2\", \"median_relative_error\", \"mean_relative_error\"], [0.007, 0.625, 0.426], [0.007, 0.346, 0.265], [0.01, 0.469, 0.335], [0.001, 0.284, 0.283], [0.01, 0.177, 0.169], [0.008, 0.369, 0.301], [0.011, 0.443, 0.317], [0.004, 0.274, 0.238]]}, \"type\": \"table\", \"uid\": \"7f847da1-03ab-11e9-bc9a-705a0f47fdfe\"}], {}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"6fd76071-b023-4c22-8797-610d38cacd15\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"6fd76071-b023-4c22-8797-610d38cacd15\", [{\"header\": {\"values\": [\"\", \"Agilent_neg\", \"Agilent_pos\", \"CBM\", \"McLean\", \"PNL\", \"Waters_neg\", \"Waters_pos\", \"global\"]}, \"cells\": {\"values\": [[\"R2\", \"median_relative_error\", \"mean_relative_error\"], [0.007, 0.625, 0.426], [0.007, 0.346, 0.265], [0.01, 0.469, 0.335], [0.001, 0.284, 0.283], [0.01, 0.177, 0.169], [0.008, 0.369, 0.301], [0.011, 0.443, 0.317], [0.004, 0.274, 0.238]]}, \"type\": \"table\", \"uid\": \"7f847da1-03ab-11e9-bc9a-705a0f47fdfe\"}], {}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_sets_names = [\"\"] + test_sets_names\n",
    "metrics = [ms_r2,  ms_median_relative, ms_mean_relative]\n",
    "averages = [[\"R2\", \"median_relative_error\", \"mean_relative_error\"]]\n",
    "stds = [[\"R2\", \"median_relative_error\", \"mean_relative_error\"]]\n",
    "for i, dset in enumerate(test_sets):\n",
    "    t_averages = []\n",
    "    t_stds = []\n",
    "    for metric in metrics:\n",
    "        t_averages.append(np.round(np.mean([d[i] for d in metric]), decimals=3))\n",
    "        t_stds.append(np.round(np.std([d[i] for d in metric]), decimals=3))\n",
    "    averages.append(t_averages)\n",
    "    stds.append(t_stds)\n",
    "    \n",
    "table_average = go.Table(\n",
    "    header=dict(values=test_sets_names),\n",
    "    cells=dict(values=averages))\n",
    "\n",
    "table_stds = go.Table(\n",
    "    header=dict(values=test_sets_names),\n",
    "    cells=dict(values=stds))\n",
    "\n",
    "iplot([table_average])\n",
    "iplot([table_stds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "cells": {
          "values": [
           [
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10
           ],
           [
            0.977,
            0.982,
            0.981,
            0.98,
            0.979,
            0.982,
            0.984,
            0.983,
            0.972,
            0.975
           ],
           [
            2.678,
            2.278,
            2.138,
            2.17,
            2.371,
            2.125,
            2.205,
            2.183,
            3.006,
            2.541
           ],
           [
            3.26,
            2.937,
            2.887,
            2.943,
            3.071,
            2.817,
            2.763,
            2.778,
            3.537,
            3.227
           ]
          ]
         },
         "header": {
          "values": [
           "Model",
           "R2",
           "Median relative",
           "mean relative"
          ]
         },
         "type": "table",
         "uid": "82fe7a10-03ab-11e9-bc9a-705a0f47fdfe"
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div id=\"5b0d5c6b-9695-44d4-be49-4cff7d69ce5c\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"5b0d5c6b-9695-44d4-be49-4cff7d69ce5c\", [{\"header\": {\"values\": [\"Model\", \"R2\", \"Median relative\", \"mean relative\"]}, \"cells\": {\"values\": [[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0], [0.977, 0.982, 0.981, 0.98, 0.979, 0.982, 0.984, 0.983, 0.972, 0.975], [2.678, 2.278, 2.138, 2.17, 2.371, 2.125, 2.205, 2.183, 3.006, 2.541], [3.26, 2.937, 2.887, 2.943, 3.071, 2.817, 2.763, 2.778, 3.537, 3.227]]}, \"type\": \"table\", \"uid\": \"82fe7a11-03ab-11e9-bc9a-705a0f47fdfe\"}], {}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"5b0d5c6b-9695-44d4-be49-4cff7d69ce5c\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"5b0d5c6b-9695-44d4-be49-4cff7d69ce5c\", [{\"header\": {\"values\": [\"Model\", \"R2\", \"Median relative\", \"mean relative\"]}, \"cells\": {\"values\": [[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0], [0.977, 0.982, 0.981, 0.98, 0.979, 0.982, 0.984, 0.983, 0.972, 0.975], [2.678, 2.278, 2.138, 2.17, 2.371, 2.125, 2.205, 2.183, 3.006, 2.541], [3.26, 2.937, 2.887, 2.943, 3.071, 2.817, 2.763, 2.778, 3.537, 3.227]]}, \"type\": \"table\", \"uid\": \"82fe7a11-03ab-11e9-bc9a-705a0f47fdfe\"}], {}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Extract the results for the global test set for each model independently in order to find the best model.\n",
    "values_to_table = [[1,2,3,4,5,6,7,8,9,10],\n",
    "                   [m[-1] for m in ms_r2], \n",
    "                   [m[-1] for m in ms_median_relative], \n",
    "                   [m[-1] for m in ms_mean_relative]]\n",
    "\n",
    "table_search_best_model = go.Table(\n",
    "    header=dict(values=[\"Model\",\"R2\", \"Median relative\", \"mean relative\"]),\n",
    "    cells=dict(values=np.round(values_to_table, decimals=3)))\n",
    "\n",
    "iplot([table_search_best_model])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset comparison\n",
    "Since predictions for the Waters datasets (Astarita et al. 2014) give results that are lower than what is observed for other dataset, we investigate the global variability of CCS measurement in this multi-source dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MetCCS_train_pos, MetCCS_train_pos, 342, 0.0\n",
      "MetCCS_train_pos, MetCCS_train_neg, 0, 0\n",
      "MetCCS_train_pos, MetCCS_test_pos, 1, 0.36862140121\n",
      "MetCCS_train_pos, MetCCS_test_neg, 0, 0\n",
      "MetCCS_train_pos, Astarita_pos, 1, 4.36169844559\n",
      "MetCCS_train_pos, Astarita_neg, 0, 0\n",
      "MetCCS_train_pos, Baker, 79, -1.77408501016\n",
      "MetCCS_train_pos, McLean, 7, -0.0337819971416\n",
      "MetCCS_train_pos, CBM, 4, 1.13823824605\n",
      "MetCCS_train_neg, MetCCS_train_pos, 0, 0\n",
      "MetCCS_train_neg, MetCCS_train_neg, 356, 0.0\n",
      "MetCCS_train_neg, MetCCS_test_pos, 0, 0\n",
      "MetCCS_train_neg, MetCCS_test_neg, 0, 0\n",
      "MetCCS_train_neg, Astarita_pos, 0, 0\n",
      "MetCCS_train_neg, Astarita_neg, 0, 0\n",
      "MetCCS_train_neg, Baker, 71, -3.73742097786\n",
      "MetCCS_train_neg, McLean, 13, -1.91652438081\n",
      "MetCCS_train_neg, CBM, 0, 0\n",
      "MetCCS_test_pos, MetCCS_train_pos, 1, -0.36862140121\n",
      "MetCCS_test_pos, MetCCS_train_neg, 0, 0\n",
      "MetCCS_test_pos, MetCCS_test_pos, 74, 0.0\n",
      "MetCCS_test_pos, MetCCS_test_neg, 0, 0\n",
      "MetCCS_test_pos, Astarita_pos, 57, 2.91256147508\n",
      "MetCCS_test_pos, Astarita_neg, 0, 0\n",
      "MetCCS_test_pos, Baker, 47, -2.39724278544\n",
      "MetCCS_test_pos, McLean, 9, -1.20282185546\n",
      "MetCCS_test_pos, CBM, 0, 0\n",
      "MetCCS_test_neg, MetCCS_train_pos, 0, 0\n",
      "MetCCS_test_neg, MetCCS_train_neg, 0, 0\n",
      "MetCCS_test_neg, MetCCS_test_pos, 0, 0\n",
      "MetCCS_test_neg, MetCCS_test_neg, 59, 0.0\n",
      "MetCCS_test_neg, Astarita_pos, 0, 0\n",
      "MetCCS_test_neg, Astarita_neg, 58, -0.22592308725\n",
      "MetCCS_test_neg, Baker, 35, -4.01843250658\n",
      "MetCCS_test_neg, McLean, 9, -3.20965486101\n",
      "MetCCS_test_neg, CBM, 0, 0\n",
      "Astarita_pos, MetCCS_train_pos, 1, -4.36169844559\n",
      "Astarita_pos, MetCCS_train_neg, 0, 0\n",
      "Astarita_pos, MetCCS_test_pos, 57, -2.91256147508\n",
      "Astarita_pos, MetCCS_test_neg, 0, 0\n",
      "Astarita_pos, Astarita_pos, 92, 0.0\n",
      "Astarita_pos, Astarita_neg, 0, 0\n",
      "Astarita_pos, Baker, 57, -5.53999143383\n",
      "Astarita_pos, McLean, 14, -4.87619674285\n",
      "Astarita_pos, CBM, 0, 0\n",
      "Astarita_neg, MetCCS_train_pos, 0, 0\n",
      "Astarita_neg, MetCCS_train_neg, 0, 0\n",
      "Astarita_neg, MetCCS_test_pos, 0, 0\n",
      "Astarita_neg, MetCCS_test_neg, 58, 0.22592308725\n",
      "Astarita_neg, Astarita_pos, 0, 0\n",
      "Astarita_neg, Astarita_neg, 117, 0.0\n",
      "Astarita_neg, Baker, 65, -4.9900834687\n",
      "Astarita_neg, McLean, 12, -2.82099802254\n",
      "Astarita_neg, CBM, 0, 0\n",
      "Baker, MetCCS_train_pos, 79, 1.77408501016\n",
      "Baker, MetCCS_train_neg, 71, 3.73742097786\n",
      "Baker, MetCCS_test_pos, 47, 2.39724278544\n",
      "Baker, MetCCS_test_neg, 35, 4.01843250658\n",
      "Baker, Astarita_pos, 57, 5.53999143383\n",
      "Baker, Astarita_neg, 65, 4.9900834687\n",
      "Baker, Baker, 878, 0.0\n",
      "Baker, McLean, 54, 0.26464732252\n",
      "Baker, CBM, 2, 3.61765253146\n",
      "McLean, MetCCS_train_pos, 7, 0.0337819971416\n",
      "McLean, MetCCS_train_neg, 13, 1.91652438081\n",
      "McLean, MetCCS_test_pos, 9, 1.20282185546\n",
      "McLean, MetCCS_test_neg, 9, 3.20965486101\n",
      "McLean, Astarita_pos, 14, 4.87619674285\n",
      "McLean, Astarita_neg, 12, 2.82099802254\n",
      "McLean, Baker, 54, -0.26464732252\n",
      "McLean, McLean, 272, 0.0\n",
      "McLean, CBM, 5, 0.384408769449\n",
      "CBM, MetCCS_train_pos, 4, -1.13823824605\n",
      "CBM, MetCCS_train_neg, 0, 0\n",
      "CBM, MetCCS_test_pos, 0, 0\n",
      "CBM, MetCCS_test_neg, 0, 0\n",
      "CBM, Astarita_pos, 0, 0\n",
      "CBM, Astarita_neg, 0, 0\n",
      "CBM, Baker, 2, -3.61765253146\n",
      "CBM, McLean, 5, -0.384408769449\n",
      "CBM, CBM, 357, 0.0\n"
     ]
    }
   ],
   "source": [
    "def compare_two_CCS_datasets(data1, data2):\n",
    "    df1 = pd.DataFrame({\"SMILES\": data1[\"SMILES\"],\n",
    "                        \"Adducts\": data1[\"Adducts\"],\n",
    "                        \"CCS1\": data1[\"CCS\"]})\n",
    "    \n",
    "    df2 = pd.DataFrame({\"SMILES\": data2[\"SMILES\"],\n",
    "                        \"Adducts\": data2[\"Adducts\"],\n",
    "                        \"CCS2\": data2[\"CCS\"]})\n",
    "\n",
    "    merged_df = pd.merge(left=df1, right=df2, on=[\"SMILES\", \"Adducts\"], how='inner')\n",
    "    n = len(merged_df[\"SMILES\"])\n",
    "    if n == 0:\n",
    "        diff = 0\n",
    "    else:\n",
    "        # (1-2)/((1+2)/2)*100 \n",
    "        ccs1 = np.array(merged_df[\"CCS1\"])\n",
    "        ccs2 = np.array(merged_df[\"CCS2\"])\n",
    "        diff = np.average((ccs1-ccs2)/((ccs1+ccs2)/2)*100)\n",
    "    return n,diff\n",
    "\n",
    "for i, d1 in enumerate(datasets_names):\n",
    "    for j, d2 in enumerate(datasets_names):\n",
    "        n_identical, diff_identical = compare_two_CCS_datasets(datasets[i], datasets[j])\n",
    "        print(\"{}, {}, {}, {}\".format(d1,d2,n_identical, diff_identical))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we suspected, the CCS measurement from the Astarita paper are more divergent from the training data than other testing set. Although the specific reason will remain unknow, we can still mention that the calibration of TWIMS instrument impact CCS measurement and that it should be carefully executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
